"""
================================================================================
KRX ìƒì¥ì‚¬ ë¬¸ì„œê²€ìƒ‰ê¸° (DeepSearch API ê¸°ë°˜)
================================================================================

[í”„ë¡œê·¸ë¨ ê°œìš”]
- ëª©ì : DeepSearch APIë¥¼ í™œìš©í•˜ì—¬ ë‰´ìŠ¤, ì¦ê¶Œì‚¬ë³´ê³ ì„œ, ê³µì‹œ/IR, íŠ¹í—ˆ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ ,
        ê²€ìƒ‰ ê²°ê³¼ì—ì„œ KRX ìƒì¥ì‚¬(KOSPI, KOSDAQ, KONEX)ê°€ ì–¸ê¸‰ëœ ë¬¸ì„œë§Œ í•„í„°ë§
- ë°ì´í„° ì†ŒìŠ¤: DeepSearch API (https://api.deepsearch.com)
- ìƒì¥ì‚¬ ì •ë³´: PostgreSQL DB (Supabase) - ds_entitysummary í…Œì´ë¸”

[ì£¼ìš” ê¸°ëŠ¥]
1. ë¬¸ì„œ ê²€ìƒ‰: ì¹´í…Œê³ ë¦¬ë³„(ë‰´ìŠ¤/ë³´ê³ ì„œ/ê³µì‹œ/íŠ¹í—ˆ) ë¬¸ì„œ ê²€ìƒ‰
2. ì¡°ê±´ í•„í„°: ì–¸ë¡ ì‚¬, í‚¤ì›Œë“œ, ê¸°ê°„ ë“± ë‹¤ì–‘í•œ ì¡°ê±´ ì„¤ì •
3. ìƒì¥ì‚¬ í•„í„°: ê²€ìƒ‰ ê²°ê³¼ì—ì„œ KRX ìƒì¥ì‚¬ ì–¸ê¸‰ ë¬¸ì„œë§Œ ì¶”ì¶œ
4. ê²°ê³¼ í‘œì‹œ: ë§¤ì¹­ëœ ìƒì¥ì‚¬ëª…ê³¼ í•¨ê»˜ ê²°ê³¼ í‘œì‹œ

[ì‹¤í–‰ ë°©ë²•]
- ë¡œì»¬: streamlit run deepsearch_query.py
- ë°°í¬: Streamlit Cloudì—ì„œ ìë™ ì‹¤í–‰
================================================================================
"""

import pandas as pd
import streamlit as st
import requests
import os
import time
import psycopg2
from datetime import datetime, timedelta
import plotly.graph_objects as go
from requests.packages.urllib3.exceptions import InsecureRequestWarning

# SSL ì¸ì¦ì„œ ê²½ê³  ë¹„í™œì„±í™” (DeepSearch APIê°€ self-signed ì¸ì¦ì„œ ì‚¬ìš© ì‹œ)
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


# ==============================================================================
# í™˜ê²½ ì„¤ì • (ë¡œì»¬/Streamlit Cloud ìë™ ê°ì§€)
# ==============================================================================
# [í™˜ê²½ ì„¤ì • ìë™ ê°ì§€]
# - Streamlit Cloud ë°°í¬ ì‹œ: st.secretsì—ì„œ ì„¤ì •ê°’ ë¡œë“œ
# - ë¡œì»¬ ê°œë°œ ì‹œ: .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
#
# [í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜/ì‹œí¬ë¦¿]
# - API_KEY: DeepSearch API ì¸ì¦í‚¤ (Basic Auth, base64 ì¸ì½”ë”©)
# - DB_HOST: PostgreSQL í˜¸ìŠ¤íŠ¸ (ì˜ˆ: db.xxxxx.supabase.co)
# - DB_PORT: PostgreSQL í¬íŠ¸ (ê¸°ë³¸ê°’: 5432)
# - DB_NAME: ë°ì´í„°ë² ì´ìŠ¤ëª… (ê¸°ë³¸ê°’: postgres)
# - DB_USER/DB_PASSWORD: ì½ê¸° ì „ìš© DB ì ‘ì† ì •ë³´
# - DB_USER_CRUD/DB_PASSWORD_CRUD: CRUD ì‘ì—…ìš© DB ì ‘ì† ì •ë³´

def get_config():
    """
    ì‹¤í–‰ í™˜ê²½ì„ ìë™ ê°ì§€í•˜ì—¬ ì ì ˆí•œ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        dict: api_key, db_config, db_config_crudë¥¼ í¬í•¨í•˜ëŠ” ì„¤ì • ë”•ì…”ë„ˆë¦¬

    ë™ì‘ ë°©ì‹:
    1. ë¨¼ì € st.secrets ì ‘ê·¼ ì‹œë„ (Streamlit Cloud í™˜ê²½)
    2. ì‹¤íŒ¨ ì‹œ .env íŒŒì¼ì—ì„œ ë¡œë“œ (ë¡œì»¬ ê°œë°œ í™˜ê²½)
    """
    try:
        # Streamlit Cloud í™˜ê²½: secrets.tomlì—ì„œ ì„¤ì • ë¡œë“œ
        config = {
            'api_key': st.secrets["general"]["api_key"],
            'db_config': {
                'user': st.secrets["general"]["db_user"],
                'password': st.secrets["general"]["db_password"],
                'host': st.secrets["general"]["db_host"],
                'port': st.secrets["general"]["db_port"],
                'database': st.secrets["general"]["db_name"],
            },
            'db_config_crud': {
                'user': st.secrets["crud"]["db_user"],
                'password': st.secrets["crud"]["db_password"],
                'host': st.secrets["crud"]["db_host"],
                'port': st.secrets["crud"]["db_port"],
                'database': st.secrets["general"]["db_name"],
            }
        }
        return config
    except Exception:
        # ë¡œì»¬ ê°œë°œ í™˜ê²½: .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
        from dotenv import load_dotenv
        load_dotenv()

        config = {
            'api_key': os.getenv("API_KEY"),
            'db_config': {
                'user': os.getenv("DB_USER"),
                'password': os.getenv("DB_PASSWORD"),
                'host': os.getenv("DB_HOST"),
                'port': os.getenv("DB_PORT"),
                'database': os.getenv("DB_NAME"),
            },
            'db_config_crud': {
                'user': os.getenv("DB_USER_CRUD"),
                'password': os.getenv("DB_PASSWORD_CRUD"),
                'host': os.getenv("DB_HOST"),
                'port': os.getenv("DB_PORT"),
                'database': os.getenv("DB_NAME"),
            }
        }
        return config


# ì„¤ì • ë¡œë“œ
config = get_config()
api_key = config['api_key']
db_config = config['db_config']
db_config_crud = config['db_config_crud']


# ==============================================================================
# API ì„¤ì •
# ==============================================================================
# [DeepSearch API ì„¤ì •]
# - ì¸ì¦ ë°©ì‹: Basic Authentication (API í‚¤ë¥¼ base64 ì¸ì½”ë”©í•˜ì—¬ ì „ì†¡)
# - ì—”ë“œí¬ì¸íŠ¸: https://api.deepsearch.com/v1/compute
# - ìš”ì²­ í˜•ì‹: GET ìš”ì²­, input íŒŒë¼ë¯¸í„°ë¡œ ì¿¼ë¦¬ í•¨ìˆ˜ ì „ë‹¬

# HTTP ìš”ì²­ í—¤ë” (ì¸ì¦ ì •ë³´ í¬í•¨)
headers = {
    'Authorization': f'Basic {api_key}'
}

# DeepSearch API ê¸°ë³¸ URL
# ì¿¼ë¦¬ í•¨ìˆ˜ëŠ” input íŒŒë¼ë¯¸í„°ë¡œ URL ì¸ì½”ë”©ë˜ì–´ ì „ë‹¬ë¨
url_base = 'https://api.deepsearch.com/v1/compute?input='


# ==============================================================================
# API ìš”ì²­ í•¨ìˆ˜
# ==============================================================================

def generate_url(base_query, page):
    """
    í˜ì´ì§€ë„¤ì´ì…˜ì„ ìœ„í•œ API ìš”ì²­ URLì„ ìƒì„±í•©ë‹ˆë‹¤.

    [ë™ì‘ ì„¤ëª…]
    DocumentSearch APIëŠ” í•œ ë²ˆì— ìµœëŒ€ 100ê±´ì˜ ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì „ì²´ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ë ¤ë©´ page íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ë©° ë°˜ë³µ ìš”ì²­í•´ì•¼ í•©ë‹ˆë‹¤.

    Args:
        base_query (str): page=1ë¡œ ì„¤ì •ëœ ê¸°ë³¸ DocumentSearch ì¿¼ë¦¬
                         ì˜ˆ: 'DocumentSearch(["news"], "í‚¤ì›Œë“œ", count=100, page=1)'
        page (int): ìš”ì²­í•  í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)

    Returns:
        str: ì™„ì„±ëœ API ìš”ì²­ URL

    Example:
        >>> base = 'DocumentSearch(["news"], "ì‚¼ì„±", count=100, page=1)'
        >>> generate_url(base, 3)
        'https://api.deepsearch.com/v1/compute?input=DocumentSearch(["news"], "ì‚¼ì„±", count=100, page=3)'
    """
    # ê¸°ë³¸ ì¿¼ë¦¬ì˜ page ê°’ì„ ì›í•˜ëŠ” í˜ì´ì§€ë¡œ êµì²´
    query = base_query.replace('page = 1', f'page = {page}')
    # URLì—ì„œ ì¤„ë°”ê¿ˆ ë¬¸ì ì œê±° (ì¿¼ë¦¬ê°€ ì—¬ëŸ¬ ì¤„ë¡œ ì‘ì„±ëœ ê²½ìš° ëŒ€ë¹„)
    return f'{url_base}{query}'.replace('\n', '')


def make_request(url, headers, max_retries=5):
    """
    DeepSearch APIì— HTTP GET ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    [ë™ì‘ ì„¤ëª…]
    ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë‚˜ ì„œë²„ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.

    [ì¬ì‹œë„ ì •ì±…]
    - ìµœëŒ€ ì¬ì‹œë„: 5íšŒ (ê¸°ë³¸ê°’)
    - ì¬ì‹œë„ ê°„ê²©: 5ì´ˆ ê³ ì •
    - ì¬ì‹œë„ ëŒ€ìƒ: ëª¨ë“  RequestException (íƒ€ì„ì•„ì›ƒ, ì—°ê²° ì˜¤ë¥˜, HTTP ì˜¤ë¥˜ ë“±)

    Args:
        url (str): API ìš”ì²­ URL
        headers (dict): HTTP ìš”ì²­ í—¤ë” (ì¸ì¦ ì •ë³´ í¬í•¨)
        max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)

    Returns:
        requests.Response: API ì‘ë‹µ ê°ì²´

    Raises:
        Exception: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ

    Note:
        verify=Falseë¡œ SSL ì¸ì¦ì„œ ê²€ì¦ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
        ì´ëŠ” DeepSearch APIì˜ ì¸ì¦ì„œ ë¬¸ì œë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
    """
    attempt = 0
    while attempt < max_retries:
        try:
            response = requests.get(url, headers=headers, verify=False)
            response.raise_for_status()  # HTTP ì˜¤ë¥˜ ìƒíƒœ ì½”ë“œ í™•ì¸ (4xx, 5xx)
            return response
        except requests.exceptions.RequestException as e:
            attempt += 1
            print(f"Request failed: {e}. Attempt {attempt} of {max_retries}. Retrying in 5 seconds...")
            time.sleep(5)
    raise Exception("Max retries exceeded")


# ==============================================================================
# ì¢…ëª© ìƒì„¸ ì •ë³´ API í•¨ìˆ˜
# ==============================================================================

def get_stock_prices(symbol, date_from, date_to, headers):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ì£¼ê°€ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        symbol (str): ì¢…ëª© ì‹¬ë³¼ (ì˜ˆ: KRX:005930)
        date_from (str): ì‹œì‘ì¼ (YYYY-MM-DD)
        date_to (str): ì¢…ë£Œì¼ (YYYY-MM-DD)
        headers (dict): API ìš”ì²­ í—¤ë”

    Returns:
        pd.DataFrame: ì£¼ê°€ ë°ì´í„° (date, open, high, low, close, volume)
    """
    import urllib.parse
    # ë‚ ì§œ íŒŒë¼ë¯¸í„° ì—†ì´ í˜¸ì¶œí•˜ë©´ ê°€ì¥ ìµœê·¼ ê±°ë˜ì¼ ë°ì´í„° ë°˜í™˜
    # ë‚ ì§œê°€ ìˆìœ¼ë©´ í•´ë‹¹ ê¸°ê°„ ì¡°íšŒ
    if date_from and date_to:
        query = f'GetStockPrices([{symbol}],date_from={date_from},date_to={date_to})'
    else:
        query = f'GetStockPrices([{symbol}])'
    encoded_query = urllib.parse.quote(query)
    url = f"https://api.deepsearch.com/v1/compute?input={encoded_query}"

    try:
        response = make_request(url, headers, max_retries=3)
        data = response.json()

        if 'data' in data and 'pods' in data['data']:
            for pod in data['data']['pods']:
                if pod.get('class') == 'Result:DataFrame' and 'content' in pod:
                    content = pod['content']
                    if 'data' in content:
                        df_data = content['data']
                        # ì‘ë‹µ í˜•ì‹: {'date': [...], 'symbol': [...], ...}
                        if isinstance(df_data, dict) and 'close' in df_data:
                            df = pd.DataFrame(df_data)
                            return df
        return pd.DataFrame()
    except Exception as e:
        print(f"ì£¼ê°€ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()


def get_disclosure_documents(symbol, date_from, date_to, headers, count=50):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ê³µì‹œ ë¬¸ì„œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        symbol (str): ì¢…ëª© ì‹¬ë³¼ (ì˜ˆ: KRX:005930)
        date_from (str): ì‹œì‘ì¼ (YYYY-MM-DD)
        date_to (str): ì¢…ë£Œì¼ (YYYY-MM-DD)
        headers (dict): API ìš”ì²­ í—¤ë”
        count (int): ì¡°íšŒí•  ë¬¸ì„œ ìˆ˜

    Returns:
        list: ê³µì‹œ ë¬¸ì„œ ëª©ë¡
    """
    import urllib.parse
    query = f'DocumentSearch(["company"],["disclosure"],"securities.symbol:{symbol}", count={count}, date_from={date_from}, date_to={date_to})'
    encoded_query = urllib.parse.quote(query)
    url = f"https://api.deepsearch.com/v1/compute?input={encoded_query}"

    try:
        response = make_request(url, headers, max_retries=3)
        data = response.json()

        if 'data' in data and 'pods' in data['data']:
            for pod in data['data']['pods']:
                if 'content' in pod and 'data' in pod['content']:
                    content_data = pod['content']['data']
                    if 'docs' in content_data:
                        return content_data['docs']
        return []
    except Exception as e:
        print(f"ê³µì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []


def get_ir_documents(symbol, date_from, date_to, headers, count=50):
    """
    íŠ¹ì • ì¢…ëª©ì˜ IR ìë£Œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        symbol (str): ì¢…ëª© ì‹¬ë³¼ (ì˜ˆ: KRX:005930)
        date_from (str): ì‹œì‘ì¼ (YYYY-MM-DD)
        date_to (str): ì¢…ë£Œì¼ (YYYY-MM-DD)
        headers (dict): API ìš”ì²­ í—¤ë”
        count (int): ì¡°íšŒí•  ë¬¸ì„œ ìˆ˜

    Returns:
        list: IR ë¬¸ì„œ ëª©ë¡
    """
    import urllib.parse
    query = f'DocumentSearch(["company"],["ir"],"securities.symbol:{symbol}", count={count}, date_from={date_from}, date_to={date_to})'
    encoded_query = urllib.parse.quote(query)
    url = f"https://api.deepsearch.com/v1/compute?input={encoded_query}"

    try:
        response = make_request(url, headers, max_retries=3)
        data = response.json()

        if 'data' in data and 'pods' in data['data']:
            for pod in data['data']['pods']:
                if 'content' in pod and 'data' in pod['content']:
                    content_data = pod['content']['data']
                    if 'docs' in content_data:
                        return content_data['docs']
        return []
    except Exception as e:
        print(f"IR ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []


def get_analyst_reports(symbol, date_from, date_to, headers, count=50):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ì• ë„ë¦¬ìŠ¤íŠ¸ ë³´ê³ ì„œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        symbol (str): ì¢…ëª© ì‹¬ë³¼ (ì˜ˆ: KRX:005930)
        date_from (str): ì‹œì‘ì¼ (YYYY-MM-DD)
        date_to (str): ì¢…ë£Œì¼ (YYYY-MM-DD)
        headers (dict): API ìš”ì²­ í—¤ë”
        count (int): ì¡°íšŒí•  ë¬¸ì„œ ìˆ˜

    Returns:
        list: ì• ë„ë¦¬ìŠ¤íŠ¸ ë³´ê³ ì„œ ëª©ë¡
    """
    import urllib.parse
    query = f'DocumentSearch(["research"],["company"],"securities.symbol:{symbol}", count={count}, date_from={date_from}, date_to={date_to})'
    encoded_query = urllib.parse.quote(query)
    url = f"https://api.deepsearch.com/v1/compute?input={encoded_query}"

    try:
        response = make_request(url, headers, max_retries=3)
        data = response.json()

        if 'data' in data and 'pods' in data['data']:
            for pod in data['data']['pods']:
                if 'content' in pod and 'data' in pod['content']:
                    content_data = pod['content']['data']
                    if 'docs' in content_data:
                        return content_data['docs']
        return []
    except Exception as e:
        print(f"ì• ë„ë¦¬ìŠ¤íŠ¸ ë³´ê³ ì„œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []


# ==============================================================================
# Streamlit UI ì´ˆê¸° ì„¤ì •
# ==============================================================================
# [UI êµ¬ì„±]
# - í˜ì´ì§€ ì œëª©: KRX ê²€ìƒ‰ê¸°
# - ë ˆì´ì•„ì›ƒ: wide (ë„“ì€ í™”ë©´ í™œìš©)
# - êµ¬ì„± ìš”ì†Œ:
#   1. ì œëª© ë° ë°ì´í„° ì—…ë°ì´íŠ¸ ì •ë³´
#   2. ê²€ìƒ‰ ì¡°ê±´ ì„¤ì • (Expander)
#   3. ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
#   4. í•„í„°ë§ ê¸°ëŠ¥

st.set_page_config(page_title='KRX ê²€ìƒ‰ê¸°', layout="wide")
st.markdown('<h1>KRX ìƒì¥ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ê¸° <span style="font-size: 0.5em; font-weight: normal;">(ë‰´ìŠ¤ê²€ìƒ‰ê³¼ ì—°ê³„í•œ ê³µì‹œ/IR/ì• ë„ë³´ê³ ì„œ/ì£¼ê°€ ë¶„ì„)</span></h1>', unsafe_allow_html=True)
st.caption('â€»ë³¸ ì„œë¹„ìŠ¤ëŠ” Deepsearchì˜ ê³µì‹ì„œë¹„ìŠ¤ê°€ ì•„ë‹ˆë©°, ì •ì¬ê´‘ ê³¼ì¥ì´ Deepsearch API ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì œì‘í•´ë³¸ ì„œë¹„ìŠ¤ ì˜ˆì‹œì…ë‹ˆë‹¤.')
st.markdown("### [ğŸ“š (ì°¸ê³ ë§í¬) DeepSearchë¥¼ KRX ì—…ë¬´ì— í™œìš©í•˜ëŠ” ë°©ì•ˆ ì˜ˆì‹œ](https://beaten-by-the-market.github.io/deepsearch/api_guide.html)")


# ==============================================================================
# ë°ì´í„°ë² ì´ìŠ¤ í•¨ìˆ˜
# ==============================================================================

def get_last_update_time():
    """
    DBì—ì„œ ìƒì¥ì‚¬ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    [ë™ì‘ ì„¤ëª…]
    ds_entitysummary í…Œì´ë¸”ì˜ last_update ì»¬ëŸ¼ì—ì„œ ìµœëŒ€ê°’ì„ ì¡°íšŒí•˜ì—¬
    ë°ì´í„°ê°€ ì–¸ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ ê°±ì‹ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

    [ë°ì´í„° ê°±ì‹  ì£¼ê¸°]
    - GitHub Actionsì—ì„œ ë§¤ì¼ ì˜¤ì „ 7ì‹œ(KST)ì— ìë™ ê°±ì‹ 
    - deepsearch_query_api.py ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

    Returns:
        str or None: "YYYYë…„ MMì›” DDì¼" í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´
                    ì¡°íšŒ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

    Example:
        >>> get_last_update_time()
        '2024ë…„ 01ì›” 15ì¼'
    """
    try:
        connection = psycopg2.connect(**db_config)
        cursor = connection.cursor()

        # last_update ì»¬ëŸ¼ì˜ ìµœëŒ€ê°’ ì¡°íšŒ (YYYYMMDD í˜•ì‹ìœ¼ë¡œ ì €ì¥ë¨)
        cursor.execute("SELECT MAX(last_update) FROM ds_entitysummary")
        result = cursor.fetchone()[0]

        cursor.close()
        connection.close()

        if result:
            # YYYYMMDD ë¬¸ìì—´ì„ í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            update_date = datetime.strptime(str(result), "%Y%m%d")
            return update_date.strftime("%Yë…„ %mì›” %dì¼")
        return None
    except Exception:
        return None


@st.cache_data
def load_data_from_db():
    """
    PostgreSQL DBì—ì„œ KRX ìƒì¥ì‚¬ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    [ë™ì‘ ì„¤ëª…]
    ds_entitysummary í…Œì´ë¸”ì—ì„œ ì „ì²´ ìƒì¥ì‚¬ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    Streamlitì˜ @st.cache_data ë°ì½”ë ˆì´í„°ë¡œ ê²°ê³¼ë¥¼ ìºì‹±í•˜ì—¬
    ì•± ì¬ì‹¤í–‰ ì‹œì—ë„ DB ì¬ì¡°íšŒ ì—†ì´ ìºì‹œëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    [í…Œì´ë¸” êµ¬ì¡°: ds_entitysummary]
    - symbol: ì¢…ëª© ì½”ë“œ (ì˜ˆ: '005930')
    - symbol_nice: NICE ì‹¬ë³¼ (ì˜ˆ: 'NICE:380725')
    - entity_name: ê¸°ì—…ëª… (ì˜ˆ: 'ì‚¼ì„±ì „ì')
    - business_rid: ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸ (í•˜ì´í”ˆ ì œê±°ëœ 10ìë¦¬)
    - company_rid: ë²•ì¸ë“±ë¡ë²ˆí˜¸ (í•˜ì´í”ˆ ì œê±°ëœ 13ìë¦¬)
    - mkt: ì‹œì¥ êµ¬ë¶„ (KOSPI, KOSDAQ, KONEX)
    - last_update: ë°ì´í„° ê°±ì‹ ì¼ (YYYYMMDD)

    [ì„±ëŠ¥ ìµœì í™”]
    - pd.read_sql_query ì‚¬ìš©ìœ¼ë¡œ í•œ ë²ˆì— ì „ì²´ ë°ì´í„° ë¡œë“œ
    - @st.cache_dataë¡œ ìºì‹±í•˜ì—¬ ë°˜ë³µ ì¡°íšŒ ë°©ì§€

    Returns:
        pd.DataFrame: ìƒì¥ì‚¬ ì •ë³´ DataFrame
                     ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ë¹ˆ DataFrame ë°˜í™˜
    """
    connection = None
    try:
        connection = psycopg2.connect(**db_config)

        if connection:
            print("Connected to PostgreSQL database")

            # pd.read_sql_queryë¡œ í•œ ë²ˆì— ì „ì²´ ë°ì´í„° ë¡œë“œ (ì„±ëŠ¥ ìµœì í™”)
            # ê¸°ì¡´: fetchmany + pd.concat ë°˜ë³µ -> ë¹„íš¨ìœ¨ì 
            # ê°œì„ : read_sql_queryë¡œ ë‹¨ì¼ í˜¸ì¶œ
            df = pd.read_sql_query("SELECT * FROM ds_entitysummary", connection)
            return df

    except psycopg2.Error as e:
        st.error(f"Database Error: {e}")
        return pd.DataFrame()

    finally:
        if connection:
            connection.close()
            print("PostgreSQL connection is closed")


# ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ í‘œì‹œ
last_update = get_last_update_time()
if last_update:
    st.caption(f"ğŸ“… ìƒì¥ì¢…ëª© ì •ë³´ ì—…ë°ì´íŠ¸: {last_update} (ë§¤ì¼ ì˜¤ì „ 7ì‹œ ìë™ ê°±ì‹ )")

# ìƒì¥ì‚¬ ë°ì´í„° ë¡œë“œ (ìºì‹±ë¨)
search_list_df_original = load_data_from_db()


# ==============================================================================
# ê²€ìƒ‰ ì¡°ê±´ ì„ íƒì§€ ì •ì˜
# ==============================================================================
# [ê²€ìƒ‰ ì¡°ê±´ ë°ì´í„° êµ¬ì¡°]
# ê° ì„ íƒì§€ëŠ” DataFrameìœ¼ë¡œ ê´€ë¦¬ë˜ë©°, ì‚¬ìš©ìê°€ ì„ íƒí•œ í•œê¸€ ë ˆì´ë¸”ì„
# API ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

# êµ­ë‚´ë‰´ìŠ¤ ì„¹ì…˜ ì„ íƒì§€
# ë‰´ìŠ¤ ë¬¸ì„œì˜ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ (section íŒŒë¼ë¯¸í„°)
domestic_news_dict = {
    'êµ­ë‚´ë‰´ìŠ¤': ['ì „ì²´', 'ê²½ì œ', 'ê¸°ìˆ /IT', 'ë¬¸í™”', 'ì‚¬ì„¤', 'ì‚¬íšŒ', 'ì„¸ê³„', 'ì—°ì˜ˆ', 'ì •ì¹˜'],
    'news': ['[""]', '["economy"]', '["tech"]', '["culture"]', '["opinion"]',
             '["society"]', '["world"]', '["entertainment"]', '["politics"]']
}
df_domestic_news = pd.DataFrame(domestic_news_dict)

# ì–¸ë¡ ì‚¬ ê·¸ë£¹ ì„ íƒì§€
# publisher.raw í•„ë“œë¥¼ ì‚¬ìš©í•œ Elasticsearch ì¿¼ë¦¬ êµ¬ë¬¸
# ê° ê·¸ë£¹ì— í¬í•¨ëœ ì–¸ë¡ ì‚¬ ëª©ë¡ì€ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆìŒ
news_comp_dict = {
    'ì–¸ë¡ ì‚¬': [
        'ì „ì²´', 'ì¤‘ì•™ì¼ê°„ì§€', 'ì¤‘ì•™ê²½ì œì§€',
        'ì¤‘ì•™ì¼ê°„ì§€ ë° ê²½ì œì§€', 'ì„ê°„ì§€', 'ì¢…í•©ì¼ê°„ì§€, ì§€ë°©ì§€'
    ],
    'publisher': [
        # ì „ì²´: ëª¨ë“  ì£¼ìš” ì–¸ë¡ ì‚¬ í¬í•¨
        "publisher.raw :('ê²½í–¥ì‹ ë¬¸' or 'êµ­ë¯¼ì¼ë³´' or 'ë™ì•„ì¼ë³´' or 'ì„œìš¸ì‹ ë¬¸' or 'ì„¸ê³„ì¼ë³´' or 'ì•„ì‹œì•„íˆ¬ë°ì´' or 'ì¡°ì„ ì¼ë³´' or 'ì¤‘ì•™ì¼ë³´' or 'í•œê²¨ë ˆ' or 'í•œêµ­ì¼ë³´' or 'ë‰´ìŠ¤í† ë§ˆí† ' or 'ë””ì§€í„¸íƒ€ì„ìŠ¤' or 'ë§¤ì¼ê²½ì œ' or 'ë¨¸ë‹ˆíˆ¬ë°ì´' or 'ì„œìš¸ê²½ì œ' or 'ì•„ì£¼ê²½ì œ' or 'ì´ë°ì¼ë¦¬' or 'ì´íˆ¬ë°ì´' or 'ì „ìì‹ ë¬¸' or 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤' or 'í•œêµ­ê²½ì œ' or 'ë‚´ì¼ì‹ ë¬¸' or 'ë¬¸í™”ì¼ë³´' or 'ì•„ì‹œì•„ê²½ì œ' or 'ì§€ì—­ë‚´ì¼ì‹ ë¬¸' or 'í—¤ëŸ´ë“œê²½ì œ' or 'ì¤‘ì†Œê¸°ì—…ë‰´ìŠ¤' or 'ë©”íŠ¸ë¡œê²½ì œ' or 'êµ­ì œì‹ ë¬¸' or 'ë¶€ì‚°ì¼ë³´')",
        # ì¤‘ì•™ì¼ê°„ì§€: ì£¼ìš” ì¢…í•© ì¼ê°„ì§€
        "publisher.raw :('ê²½í–¥ì‹ ë¬¸' or 'êµ­ë¯¼ì¼ë³´' or 'ë™ì•„ì¼ë³´' or 'ì„œìš¸ì‹ ë¬¸' or 'ì„¸ê³„ì¼ë³´' or 'ì•„ì‹œì•„íˆ¬ë°ì´' or 'ì¡°ì„ ì¼ë³´' or 'ì¤‘ì•™ì¼ë³´' or 'í•œê²¨ë ˆ' or 'í•œêµ­ì¼ë³´')",
        # ì¤‘ì•™ê²½ì œì§€: ê²½ì œ ì „ë¬¸ ì¼ê°„ì§€
        "publisher.raw :('ë‰´ìŠ¤í† ë§ˆí† ' or 'ë””ì§€í„¸íƒ€ì„ìŠ¤' or 'ë§¤ì¼ê²½ì œ' or 'ë¨¸ë‹ˆíˆ¬ë°ì´' or 'ì„œìš¸ê²½ì œ' or 'ì•„ì£¼ê²½ì œ' or 'ì´ë°ì¼ë¦¬' or 'ì´íˆ¬ë°ì´' or 'ì „ìì‹ ë¬¸' or 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤' or 'í•œêµ­ê²½ì œ')",
        # ì¤‘ì•™ì¼ê°„ì§€ ë° ê²½ì œì§€: ìœ„ ë‘ ê·¸ë£¹ í•©ì§‘í•©
        "publisher.raw :('ê²½í–¥ì‹ ë¬¸' or 'êµ­ë¯¼ì¼ë³´' or 'ë™ì•„ì¼ë³´' or 'ì„œìš¸ì‹ ë¬¸' or 'ì„¸ê³„ì¼ë³´' or 'ì•„ì‹œì•„íˆ¬ë°ì´' or 'ì¡°ì„ ì¼ë³´' or 'ì¤‘ì•™ì¼ë³´' or 'í•œê²¨ë ˆ' or 'í•œêµ­ì¼ë³´' or 'ë‰´ìŠ¤í† ë§ˆí† ' or 'ë””ì§€í„¸íƒ€ì„ìŠ¤' or 'ë§¤ì¼ê²½ì œ' or 'ë¨¸ë‹ˆíˆ¬ë°ì´' or 'ì„œìš¸ê²½ì œ' or 'ì•„ì£¼ê²½ì œ' or 'ì´ë°ì¼ë¦¬' or 'ì´íˆ¬ë°ì´' or 'ì „ìì‹ ë¬¸' or 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤' or 'í•œêµ­ê²½ì œ')",
        # ì„ê°„ì§€
        "publisher.raw :('ë‚´ì¼ì‹ ë¬¸' or 'ë¬¸í™”ì¼ë³´' or 'ì•„ì‹œì•„ê²½ì œ' or 'ì§€ì—­ë‚´ì¼ì‹ ë¬¸' or 'í—¤ëŸ´ë“œê²½ì œ')",
        # ì¢…í•©ì¼ê°„ì§€, ì§€ë°©ì§€
        "publisher.raw :('ë©”íŠ¸ë¡œê²½ì œ' or 'êµ­ì œì‹ ë¬¸' or 'ë¶€ì‚°ì¼ë³´')"
    ]
}
df_news_comp = pd.DataFrame(news_comp_dict)

# ì‹œì¥ êµ¬ë¶„ ì„ íƒì§€ (í•„í„°ë§ìš© ë©€í‹°ì…€ë ‰íŠ¸)
MARKET_OPTIONS = {
    'ìœ ê°€ì¦ê¶Œ': 'KOSPI',
    'ì½”ìŠ¤ë‹¥': 'KOSDAQ',
    'ì½”ë„¥ìŠ¤': 'KONEX'
}

# ì´ìŠˆ ì¹´í…Œê³ ë¦¬ í”„ë¦¬ì…‹ ì •ì˜
# ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê´€ë ¨ í‚¤ì›Œë“œ ì¡°í•©ì„ ë¯¸ë¦¬ ì •ì˜
# ê²€ìƒ‰ í›„ í•„í„°ë§ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë¨
ISSUE_CATEGORIES = {
    'ì „ì²´': None,  # í•„í„° ì—†ìŒ
    'ê³„ì•½/ìˆ˜ì£¼': '(ìˆ˜ì£¼ and ì²´ê²°) or (ìˆ˜ì£¼ and ê³µê¸‰) or (ê³„ì•½ and ì²´ê²°) or (ê³„ì•½ and ê³µê¸‰)',
    'ì¸ìˆ˜/íˆ¬ì': 'ì¸ìˆ˜ or í•©ë³‘ or ë¶„í•  or ì˜ì—…ì–‘ë„ or ì˜ì—…ì–‘ìˆ˜ or ì— ì•¤ì—ì´ or ì¶œì or íˆ¬ì',
    'ì‹¤ì ': '(ë§¤ì¶œ and ë°œí‘œ) or (ë§¤ì¶œ and ê³µí‘œ) or (ë§¤ì¶œ and ê²°ì •) or (ë§¤ì¶œ and ê¸°ë¡) or (ë§¤ì¶œ and ë‹¬ì„±) or (ë§¤ì¶œ and ê³µì‹œ) or (ì‹¤ì  and ë°œí‘œ) or (ì‹¤ì  and ê³µí‘œ) or (ì‹¤ì  and ê²°ì •) or (ì‹¤ì  and ê¸°ë¡) or (ì‹¤ì  and ë‹¬ì„±) or (ì‹¤ì  and ê³µì‹œ) or (ì´ìµ and ë°œí‘œ) or (ì´ìµ and ê³µí‘œ) or (ì´ìµ and ê²°ì •) or (ì´ìµ and ê¸°ë¡) or (ì´ìµ and ë‹¬ì„±) or (ì´ìµ and ê³µì‹œ) or (ë°°ë‹¹ and ë°œí‘œ) or (ë°°ë‹¹ and ê³µí‘œ) or (ë°°ë‹¹ and ê²°ì •) or (ë°°ë‹¹ and ê¸°ë¡) or (ë°°ë‹¹ and ë‹¬ì„±) or (ë°°ë‹¹ and ê³µì‹œ)',
    'ì¦ì/ê°ì': 'ì¦ì or ê°ì or ì£¼ì‹êµí™˜ or ì£¼ì‹ì´ì „ or ìš°íšŒìƒì¥',
    'íšŒê³„/ê°ì‚¬': 'ìƒì¥íì§€ or ê´€ë¦¬ì¢…ëª© or ìë³¸ì ì‹ or (ë¹„ì ì • and ê°ì‚¬) or (ë¹„ì ì • and íšŒê³„ë²•ì¸) or (ì˜ê²¬ê±°ì ˆ and ê°ì‚¬) or (ì˜ê²¬ê±°ì ˆ and íšŒê³„ë²•ì¸) or (íšŒê³„ì²˜ë¦¬ and ìœ„ë°˜) or ë¶„ì‹',
    'ì†Œì†¡/ë¶€ë„/íšŒìƒ': 'ì†Œì†¡ or íš¡ë ¹ or ë°°ì„ or ë¶€ë„ or íŒŒì‚° or íšŒìƒ or (ê³µì†Œ and ëŒ€í‘œì´ì‚¬) or (ê³µì†Œ and ì„ì›) or (ê³µì†Œ and ì´ì‚¬) or (ê¸°ì†Œ and ëŒ€í‘œì´ì‚¬) or (ê¸°ì†Œ and ì„ì›) or (ê¸°ì†Œ and ì´ì‚¬) or (í˜ì˜ and ëŒ€í‘œì´ì‚¬) or (í˜ì˜ and ì„ì›) or (í˜ì˜ and ì´ì‚¬)'
}

# ì´ìŠˆ ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (í•„í„°ë§ìš©)
def parse_keywords_from_query(query_str):
    """ì¿¼ë¦¬ ë¬¸ìì—´ì—ì„œ í‚¤ì›Œë“œ ëª©ë¡ ì¶”ì¶œ"""
    if query_str is None:
        return []
    # orë¡œ ë¶„ë¦¬í•˜ê³  ì •ë¦¬
    keywords = query_str.split(" or ")
    return [kw.strip() for kw in keywords]

ISSUE_KEYWORD_LISTS = {
    cat: parse_keywords_from_query(query)
    for cat, query in ISSUE_CATEGORIES.items()
}


# ==============================================================================
# ê²€ìƒ‰ ì¡°ê±´ ì„¤ì • UI
# ==============================================================================
# [ê²€ìƒ‰ ì¡°ê±´ UI êµ¬ì„±]
# ì‚¬ìš©ìê°€ ê²€ìƒ‰ ì¡°ê±´ì„ ì„¤ì •í•˜ëŠ” ì˜ì—­ì…ë‹ˆë‹¤.
#
# [ì…ë ¥ í•­ëª©]
# 1. ì–¸ë¡ ì‚¬ êµ¬ë¶„ / ë‰´ìŠ¤ ì„¹ì…˜ (í•œ í–‰)
# 2. ì¶”ê°€í•  ì–¸ë¡ ì‚¬ ì…ë ¥
# 3. ì„ íƒëœ ì–¸ë¡ ì‚¬ í‘œì‹œ
# 4. ê¸°ê°„ ì„¤ì •: ë‚ ì§œ ê¸°ì¤€ ë˜ëŠ” ë‚ ì§œ+ì‹œê°„ ê¸°ì¤€
#
# [Session State ì‚¬ìš©]
# Streamlitì€ UI ìƒí˜¸ì‘ìš© ì‹œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ,
# ì‚¬ìš©ì ì„ íƒê°’ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ st.session_stateë¥¼ í™œìš©í•©ë‹ˆë‹¤.

with st.expander("ğŸ” ê²€ìƒ‰ ì¡°ê±´", expanded=True):

    # --------------------------------------------------------------------------
    # 1. ì–¸ë¡ ì‚¬/ì„¹ì…˜ ì„ íƒ (í•œ í–‰ìœ¼ë¡œ êµ¬ì„±)
    # --------------------------------------------------------------------------
    st.subheader('ì–¸ë¡ ì‚¬ ë° ì„¹ì…˜')

    col_pub, col_sec = st.columns(2)

    with col_pub:
        # ì–¸ë¡ ì‚¬ ê·¸ë£¹ ì„ íƒ
        news_comp_selection = st.selectbox('ì–¸ë¡ ì‚¬ êµ¬ë¶„', df_news_comp['ì–¸ë¡ ì‚¬'])

    with col_sec:
        # ë‰´ìŠ¤ ì„¹ì…˜ ì„ íƒ
        domestic_news_selection = st.selectbox('ë‰´ìŠ¤ ì„¹ì…˜', df_domestic_news['êµ­ë‚´ë‰´ìŠ¤'])

    domestic_news_query = df_domestic_news[df_domestic_news['êµ­ë‚´ë‰´ìŠ¤'] == domestic_news_selection]['news'].values[0]

    # ì „ì²´ ì–¸ë¡ ì‚¬ ëª©ë¡ ì¶”ì¶œ (multiselectì˜ ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš©)
    all_publishers = df_news_comp[df_news_comp['ì–¸ë¡ ì‚¬'] == 'ì „ì²´']['publisher'].values[0]
    all_publisher_list = all_publishers.replace("publisher.raw :(", "").replace(")", "").replace("'", "").split(" or ")

    # Session State: ì–¸ë¡ ì‚¬ ê·¸ë£¹ ë³€ê²½ ì‹œ ì„ íƒ ëª©ë¡ ì´ˆê¸°í™”
    if 'last_news_comp_selection' not in st.session_state or st.session_state.last_news_comp_selection != news_comp_selection:
        # ì„ íƒëœ ê·¸ë£¹ì˜ ì–¸ë¡ ì‚¬ ëª©ë¡ ì¶”ì¶œ
        publishers = df_news_comp[df_news_comp['ì–¸ë¡ ì‚¬'] == news_comp_selection]['publisher'].values[0]
        publisher_list = publishers.replace("publisher.raw :(", "").replace(")", "").replace("'", "").split(" or ")

        st.session_state.publisher_options = all_publisher_list
        # ì „ì²´ ì„ íƒ ì‹œ ëª¨ë“  ì–¸ë¡ ì‚¬ë¥¼ ë””í´íŠ¸ë¡œ
        if news_comp_selection == 'ì „ì²´':
            st.session_state.selected_publishers = all_publisher_list.copy()
            st.session_state.publisher_multiselect = all_publisher_list.copy()
        else:
            st.session_state.selected_publishers = publisher_list
            st.session_state.publisher_multiselect = publisher_list.copy()
        st.session_state.last_news_comp_selection = news_comp_selection

    # ì „ì²´ ì„ íƒ ì‹œ: ì•ˆë‚´ ë©”ì‹œì§€ì™€ ì–¸ë¡ ì‚¬ ëª©ë¡ expander í‘œì‹œ
    # ê·¸ ì™¸: ê¸°ì¡´ multiselect ë°©ì‹ ìœ ì§€
    if news_comp_selection == 'ì „ì²´':
        st.info('ì „ì²´ ì–¸ë¡ ì‚¬(ì•½ 575ê°œ)ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.')

        # ì–¸ë¡ ì‚¬ ìƒì„¸ ì •ë³´ expander (ì ‘íŒ ìƒíƒœ)
        with st.expander('Deepsearch ì œê³µ ì–¸ë¡ ì‚¬ ìƒì„¸ ë° [ë¹…ì¹´ì¸ì¦ˆ(BIG KINDS)](https://www.bigkinds.or.kr) ë¹„êµ'):
            st.markdown('''
| êµ¬ë¶„ | ë”¥ì„œì¹˜ | ë¹…ì¹´ì¸ì¦ˆ |
|:---:|:---:|:---:|
| ì´ ì–¸ë¡ ì‚¬ | ì•½ 575ê°œ | 104ê°œ |
| ì¤‘ë³µ | 45ê°œ | 45ê°œ |
| ë…ì  | 529ê°œ | 59ê°œ |

**ë”¥ì„œì¹˜ ë…ì :** ì—°í•©ë‰´ìŠ¤, ë‰´ìŠ¤1, ë‰´ì‹œìŠ¤, JTBC, ì±„ë„A, TVì¡°ì„ , MBN, ì¡°ì„ ë¹„ì¦ˆ ë“±
**ë¹…ì¹´ì¸ì¦ˆ ë…ì  (ë”¥ì„œì¹˜ ë¯¸ì œê³µ):** ì•„ì‹œì•„íˆ¬ë°ì´, ì•„ì£¼ê²½ì œ, ì´íˆ¬ë°ì´, ëŒ€í•œê²½ì œ, ë¸Œë¦¿ì§€ê²½ì œ, OBS, ì§€ì—­ì¼ë³´ ë‹¤ìˆ˜
''')
            # ë”¥ì„œì¹˜ ì–¸ë¡ ì‚¬ ëª©ë¡
            st.markdown('---')
            st.caption('ë”¥ì„œì¹˜ ì œê³µ ì–¸ë¡ ì‚¬ ëª©ë¡ (ì•½ 575ê°œ)')
            try:
                import os
                publishers_file = os.path.join(os.path.dirname(__file__), 'publishers_575.txt')
                with open(publishers_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                cols = st.columns(3)
                for i, line in enumerate(lines):
                    name = line.strip()
                    if name:
                        cols[i % 3].write(f"{i+1}. {name}")
            except Exception as e:
                st.write("ì–¸ë¡ ì‚¬ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì „ì²´ ì„ íƒ ì‹œ ì¿¼ë¦¬ëŠ” ë¹ˆ ë¬¸ìì—´ (í•„í„° ì—†ìŒ)
        news_comp_query = ''
        selected_publishers = []
    else:
        # ì–¸ë¡ ì‚¬ ì¶”ê°€ ì½œë°± í•¨ìˆ˜
        def add_publisher_callback():
            publisher = st.session_state.add_publisher_input
            if publisher:
                if publisher not in st.session_state.publisher_options:
                    st.session_state.publisher_options.append(publisher)
                # multiselect ìƒíƒœì— ì§ì ‘ ì¶”ê°€
                if 'publisher_multiselect' in st.session_state:
                    current_selected = list(st.session_state.publisher_multiselect)
                    if publisher not in current_selected:
                        current_selected.append(publisher)
                    st.session_state.publisher_multiselect = current_selected
                # ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”
                st.session_state.add_publisher_input = ''

        # ì‚¬ìš©ì ì •ì˜ ì–¸ë¡ ì‚¬ ì¶”ê°€
        st.text_input(
            "ì¶”ê°€í•  ì–¸ë¡ ì‚¬ ì…ë ¥",
            key='add_publisher_input',
            on_change=add_publisher_callback
        )

        # ì–¸ë¡ ì‚¬ multiselect (session stateë¡œ ê°’ ê´€ë¦¬)
        selected_publishers = st.multiselect(
            'ì„ íƒëœ ì–¸ë¡ ì‚¬',
            options=st.session_state.publisher_options,
            key='publisher_multiselect'
        )
        st.session_state.selected_publishers = selected_publishers

        # ì„ íƒëœ ì–¸ë¡ ì‚¬ë¥¼ API ì¿¼ë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        # í˜•ì‹: publisher.raw :('ì–¸ë¡ ì‚¬1' or 'ì–¸ë¡ ì‚¬2' or ...)
        if selected_publishers:
            news_comp_query = " or ".join([f"'{publisher}'" for publisher in selected_publishers])
            news_comp_query = f"publisher.raw :({news_comp_query})"
        else:
            news_comp_query = ''

    # --------------------------------------------------------------------------
    # 2. ë‚ ì§œ/ì‹œê°„ ì„¤ì •
    # --------------------------------------------------------------------------
    # [ê¸°ê°„ ì„¤ì •]
    # ê²€ìƒ‰í•  ë¬¸ì„œì˜ ê¸°ê°„ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    #
    # [ë‚ ì§œ ê¸°ì¤€]
    # - date_from, date_to íŒŒë¼ë¯¸í„° ì‚¬ìš©
    # - í˜•ì‹: YYYYMMDD
    # - ë¬¸ì„œì˜ ë°œí–‰ì¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
    #
    # [ë‚ ì§œ ë° ì‹œê°„ ê¸°ì¤€]
    # - created_at í•„ë“œì— ëŒ€í•œ ë²”ìœ„ ì¿¼ë¦¬
    # - í˜•ì‹: YYYY-MM-DDTHH:MM:SS
    # - ë” ì •ë°€í•œ ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ ê°€ëŠ¥
    st.subheader("ë‚ ì§œ ë° ì‹œê°„ ì„¤ì •")

    # ë‚ ì§œ ê¸°ì¤€ ì„ íƒ ë¼ë””ì˜¤ ë²„íŠ¼
    date_option = st.radio(
        'ê¸°ê°„ í•„í„°',
        options=['ë‚ ì§œê¸°ì¤€', 'ë‚ ì§œ+ì‹œê°„ê¸°ì¤€'],
        horizontal=True,
        key='date_option_radio'
    )

    use_date = (date_option == 'ë‚ ì§œê¸°ì¤€')
    use_datetime = (date_option == 'ë‚ ì§œ+ì‹œê°„ê¸°ì¤€')

    # ë‚ ì§œ ê¸°ì¤€ ì…ë ¥ UI
    if use_date:
        date_col1, date_col2 = st.columns(2)
        with date_col1:
            start_date = st.date_input('ì‹œì‘ì¼', key='start_date')
        with date_col2:
            end_date = st.date_input('ì¢…ë£Œì¼', key='end_date')

        if start_date and end_date:
            # DocumentSearchì˜ date_from, date_to íŒŒë¼ë¯¸í„° í˜•ì‹
            date_query = f'date_from={start_date.strftime("%Y%m%d")} , date_to={end_date.strftime("%Y%m%d")}'
        else:
            date_query = ''
    else:
        date_query = ''

    # ë‚ ì§œ+ì‹œê°„ ê¸°ì¤€ ì…ë ¥ UI
    if use_datetime:
        datetime_col1, datetime_col2 = st.columns(2)
        with datetime_col1:
            datetime_start_date = st.date_input('ì‹œì‘ ë‚ ì§œ', key='datetime_start_date')
            datetime_start_time = st.slider('ì‹œì‘ ì‹œê°„', 0, 24, 0, key='datetime_start_time')
        with datetime_col2:
            datetime_end_date = st.date_input('ì¢…ë£Œ ë‚ ì§œ', key='datetime_end_date')
            datetime_end_time = st.slider('ì¢…ë£Œ ì‹œê°„', 0, 24, 0, key='datetime_end_time')

        if datetime_start_date and datetime_end_date:
            # created_at í•„ë“œì— ëŒ€í•œ ë²”ìœ„ ì¿¼ë¦¬ í˜•ì‹
            datetime_start = f"{datetime_start_date}T{datetime_start_time:02}:00:00"
            datetime_end = f"{datetime_end_date}T{datetime_end_time:02}:00:00"
            datetime_query = f'created_at:[\\"{datetime_start}\\" to \\"{datetime_end}\\"]'
        else:
            datetime_query = ''
    else:
        datetime_query = ''

    # ê²€ìƒ‰ ë²„íŠ¼
    search_clicked = st.button('ğŸ” ê²€ìƒ‰', type='primary', use_container_width=True)


# ==============================================================================
# ê²€ìƒ‰ ì‹¤í–‰
# ==============================================================================
# [ê²€ìƒ‰ ì‹¤í–‰ ë¡œì§]
# 1. ì‚¬ìš©ìê°€ ì„¤ì •í•œ ì¡°ê±´ë“¤ì„ ì¡°í•©í•˜ì—¬ DocumentSearch ì¿¼ë¦¬ ìƒì„±
# 2. API í˜¸ì¶œí•˜ì—¬ ì²« í˜ì´ì§€ ê²°ê³¼ íšë“
# 3. ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸ í›„ ë‚˜ë¨¸ì§€ í˜ì´ì§€ ìˆœì°¨ ìš”ì²­
# 4. ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³‘í•©í•˜ì—¬ session_stateì— ì €ì¥
#
# [DocumentSearch ì¿¼ë¦¬ êµ¬ì¡°]
# DocumentSearch(
#     ["news"],            # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ê³ ì •
#     sections,            # ì„¸ë¶€ ì„¹ì…˜ (ì„ íƒì )
#     "search_query",      # ê²€ìƒ‰ì–´ ë° í•„í„° ì¡°ê±´
#     date_from=YYYYMMDD,  # ì‹œì‘ì¼ (ì„ íƒì )
#     date_to=YYYYMMDD,    # ì¢…ë£Œì¼ (ì„ íƒì )
#     count=100,           # í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
#     page=N               # í˜ì´ì§€ ë²ˆí˜¸
# )

if search_clicked:
    # ê¸°ê°„ í•„ìˆ˜ ì²´í¬
    if not use_date and not use_datetime:
        st.error("ê¸°ê°„ì€ ë°˜ë“œì‹œ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ì§§ì„ ìˆ˜ë¡ ë¹¨ë¦¬ ê²€ìƒ‰ë©ë‹ˆë‹¤.")
    else:
        # ì¿¼ë¦¬ íŒŒíŠ¸ ì¡°í•©
        # query_parts: ì¹´í…Œê³ ë¦¬, ì„¹ì…˜ ë“± ì‰¼í‘œë¡œ êµ¬ë¶„ë˜ëŠ” íŒŒë¼ë¯¸í„°
        # query_parts_and: ê²€ìƒ‰ì–´ ë¶€ë¶„ (andë¡œ ì—°ê²°)
        # query_parts_comma: date_from, date_to ë“± ì‰¼í‘œë¡œ êµ¬ë¶„ë˜ëŠ” íŒŒë¼ë¯¸í„°
        query_parts = ['["news"]']  # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ê³ ì •
        query_parts_and = []
        query_parts_comma = []

        # ë‰´ìŠ¤ ì„¹ì…˜ ì¡°ê±´ ì¶”ê°€
        if domestic_news_query:
            query_parts.append(domestic_news_query)

        # ì–¸ë¡ ì‚¬ ì¡°ê±´ ì¶”ê°€
        if news_comp_query:
            query_parts_and.append(news_comp_query)

        # ë‚ ì§œ ì¡°ê±´ ì¶”ê°€
        if date_query:
            query_parts_comma.append(date_query)

        # ë‚ ì§œ+ì‹œê°„ ì¡°ê±´ ì¶”ê°€
        if datetime_query:
            query_parts_and.append(datetime_query)

        # None ê°’ ë° ë¹ˆ ë¬¸ìì—´ ì œê±°
        query_parts = [part for part in query_parts if part and part != 'None']
        query_parts_and = [part for part in query_parts_and if part and part != 'None']
        query_parts_comma = [part for part in query_parts_comma if part and part != 'None']

        # ìµœì¢… ì¿¼ë¦¬ ì¡°í•©
        intro = 'DocumentSearch('
        outro = ', count = 100, page = 1)'
        final_query_category = ' , '.join(query_parts)
        final_query_condition = ' and '.join(query_parts_and)
        final_query_comma = ' , '.join(query_parts_comma)

        # ì™„ì„±ëœ DocumentSearch ì¿¼ë¦¬
        final_query_all = (
            intro +
            final_query_category +
            (' , "' + final_query_condition + '"' if final_query_condition else '') +
            ((' , ' if final_query_comma else '') + final_query_comma) +
            outro
        )

        # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
        current_page = 1
        url = generate_url(final_query_all, current_page)

        # ì²« í˜ì´ì§€ ìš”ì²­
        response = make_request(url, headers)
        response_data = response.json()

        # API ì‘ë‹µì—ì„œ ë¬¸ì„œ ë°ì´í„° ì¶”ì¶œ
        # ì‘ë‹µ êµ¬ì¡°: data.pods[1].content.data.docs
        docs = response_data['data']['pods'][1]['content']['data']['docs']
        df_list = [pd.json_normalize(docs)]

        # ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
        last_page = response_data['data']['pods'][1]['content']['data']['last_page']

        # ì§„í–‰ë¥  í‘œì‹œ
        st.caption('ğŸ“¡ DeepSearch API í˜¸ì¶œì¤‘ì…ë‹ˆë‹¤. (í•˜ë£¨ ê¸°ì¤€ ì•½ 1ë¶„ ì†Œìš”)')
        progress_bar = st.progress(0)

        # ë‚˜ë¨¸ì§€ í˜ì´ì§€ ìˆœì°¨ ìš”ì²­
        while current_page < last_page:
            current_page += 1
            url = generate_url(final_query_all, current_page)
            response = make_request(url, headers)
            response_data = response.json()

            docs = response_data['data']['pods'][1]['content']['data']['docs']
            df_list.append(pd.json_normalize(docs))

            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress = int(current_page / last_page * 100)
            progress_bar.progress(progress)

        # ì „ì²´ ê²°ê³¼ ë³‘í•©
        df = pd.concat(df_list, ignore_index=True)

        # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
        df_show = df.loc[:, ~df.columns.duplicated()]

        # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        if not df.empty and all(col in df.columns for col in ['section', 'publisher', 'author', 'title', 'content', 'content_url']):
            df_show = df[['section', 'publisher', 'author', 'title', 'content', 'content_url']]
            count = len(df_show)
            st.success(f"ì´ {count}ê±´ì˜ ë‰´ìŠ¤ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            st.warning("ì„ íƒí•œ ê¸°ê°„ì— í•´ë‹¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê¸°ê°„ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.")

        # ê²°ê³¼ë¥¼ session_stateì— ì €ì¥ (í•„í„°ë§ì—ì„œ ì‚¬ìš©)
        st.session_state.df = df


# ==============================================================================
# ê²°ê³¼ í•„í„°ë§ ê¸°ëŠ¥
# ==============================================================================
# [í•„í„°ë§ ë¡œì§]
# ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
#
# [í•„í„° ì¡°ê±´]
# 1. ì‹œì¥: KOSPI/KOSDAQ/KONEX (ë©€í‹°ì…€ë ‰íŠ¸)
# 2. ì¢…ëª©: íŠ¹ì • ì¢…ëª©ëª… ê²€ìƒ‰ (ìë™ì™„ì„±)
# 3. ì´ìŠˆ ì¹´í…Œê³ ë¦¬: ê³„ì•½/ìˆ˜ì£¼, ì¸ìˆ˜/íˆ¬ì, ì‹¤ì  ë“±
#
# [ìƒì¥ì‚¬ ë§¤ì¹­ ë°©ì‹]
# ë¬¸ì„œì˜ securities, entities, named_entities í•„ë“œì—ì„œ ë§¤ì¹­

if 'df' in st.session_state:
    df = st.session_state.df

    with st.expander("ğŸ“‹ ê²°ê³¼ í•„í„°", expanded=True):

        # ----------------------------------------------------------------------
        # 1. ì‹œì¥/ì¢…ëª© í•„í„° (ë¼ë””ì˜¤ ë²„íŠ¼ìœ¼ë¡œ ì„ íƒ)
        # ----------------------------------------------------------------------
        col_title1, col_help1 = st.columns([10, 1])
        with col_title1:
            st.subheader('ì‹œì¥ ë˜ëŠ” ì¢…ëª©')
        with col_help1:
            with st.popover('â„¹ï¸'):
                st.markdown('DeepSearchê°€ ìì—°ì–´ì²˜ë¦¬ë¥¼ í†µí•´ ê¸°ì‚¬ì£¼ì œê°€ í•´ë‹¹ì¢…ëª©ì— ëŒ€í•œ ê²ƒìœ¼ë¡œ ì‹ë³„í•œ ê¸°ì‚¬ë¥¼ í•„í„°í•©ë‹ˆë‹¤. ä¾‹) ìœ ê°€ ìƒì¥ì‚¬ "ëŒ€ìƒ", "ë‚¨ì„±"ì€ ë‹¨ìˆœíˆ "ëŒ€ìƒ", "ë‚¨ì„±"ì´ ìˆìœ¼ë©´ ë§¤ì¹­í•˜ì§€ ì•Šê³ , ê¸°ì‚¬ê°€ í•´ë‹¹ ìƒì¥ì‚¬ì— ëŒ€í•œ ê²ƒì¼ ë•Œ ì‹ë³„ë©ë‹ˆë‹¤.')

        filter_type = st.radio(
            'í•„í„° ìœ í˜•',
            options=['ì‹œì¥ë³„ í•„í„°', 'ì¢…ëª©ë³„ í•„í„°', 'ë‚´ ê´€ì‹¬ì¢…ëª©'],
            horizontal=True,
            key='filter_type'
        )

        if filter_type == 'ì‹œì¥ë³„ í•„í„°':
            # ì‹œì¥ í•„í„°
            selected_markets = st.multiselect(
                'ì‹œì¥ ì„ íƒ',
                options=list(MARKET_OPTIONS.keys()),
                default=list(MARKET_OPTIONS.keys()),
                key='filter_markets'
            )
            selected_market_codes = [MARKET_OPTIONS[m] for m in selected_markets]
            # ì¢…ëª© í•„í„° ì´ˆê¸°í™”
            st.session_state.selected_stocks = []

        elif filter_type == 'ì¢…ëª©ë³„ í•„í„°':
            # ì¢…ëª©ë³„ í•„í„°
            selected_market_codes = list(MARKET_OPTIONS.values())  # ì „ì²´ ì‹œì¥ì—ì„œ ê²€ìƒ‰

            col_stock_input, col_stock_select = st.columns([1, 1])

            with col_stock_input:
                stock_search_input = st.text_input(
                    'ì¢…ëª©ëª… ì…ë ¥',
                    key='stock_search_input',
                    placeholder='ì˜ˆ: ì‚¼ì„±ì „ì'
                )
                st.caption('â†’ ì¢…ëª©ëª…ì„ ì…ë ¥í•˜ë©´ ì˜¤ë¥¸ìª½ì—ì„œ ì¢…ëª©ì½”ë“œ í™•ì¸ í›„ ì„ íƒ')

            with col_stock_select:
                if stock_search_input:
                    filtered_stocks = search_list_df_original[
                        search_list_df_original['entity_name'].str.contains(stock_search_input, case=False, na=False)
                    ][['symbol', 'entity_name', 'mkt']].head(10)

                    if not filtered_stocks.empty:
                        stock_options = [f"{row['entity_name']} ({row['symbol']}, {row['mkt']})"
                                        for _, row in filtered_stocks.iterrows()]

                        selected_stock_display = st.selectbox(
                            'ì¢…ëª© ì„ íƒ',
                            options=['ì„ íƒ ì•ˆí•¨'] + stock_options,
                            key='stock_select'
                        )

                        if selected_stock_display != 'ì„ íƒ ì•ˆí•¨':
                            selected_stock_name = selected_stock_display.split(' (')[0]
                            if 'selected_stocks' not in st.session_state:
                                st.session_state.selected_stocks = []
                            if selected_stock_name not in st.session_state.selected_stocks:
                                st.session_state.selected_stocks.append(selected_stock_name)
                    else:
                        st.selectbox('ì¢…ëª© ì„ íƒ', options=['ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ'], disabled=True, key='stock_select_empty')
                else:
                    st.selectbox('ì¢…ëª© ì„ íƒ', options=['ì¢…ëª©ëª…ì„ ì…ë ¥í•˜ì„¸ìš”'], disabled=True, key='stock_select_placeholder')

            # ì„ íƒëœ ì¢…ëª© í‘œì‹œ ë° ì œê±°
            if 'selected_stocks' in st.session_state and st.session_state.selected_stocks:
                st.caption('ì„ íƒëœ ì¢…ëª©:')
                stock_cols = st.columns(min(len(st.session_state.selected_stocks), 5))
                stocks_to_remove = []
                for i, stock in enumerate(st.session_state.selected_stocks):
                    with stock_cols[i % 5]:
                        if st.button(f'âŒ {stock}', key=f'remove_{stock}'):
                            stocks_to_remove.append(stock)

                for stock in stocks_to_remove:
                    st.session_state.selected_stocks.remove(stock)
                    st.rerun()

        else:  # ë‚´ ê´€ì‹¬ì¢…ëª©
            selected_market_codes = list(MARKET_OPTIONS.values())  # ì „ì²´ ì‹œì¥ì—ì„œ ê²€ìƒ‰

            # ì—…ë¡œë“œ ì„œì‹ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            import io
            template_df = pd.DataFrame({'ì¢…ëª©ì½”ë“œ': ['005930', '000660', '035720']})
            buffer = io.BytesIO()
            template_df.to_excel(buffer, index=False, engine='openpyxl')
            buffer.seek(0)

            col_download, col_upload = st.columns([1, 2])

            with col_download:
                st.download_button(
                    label='ğŸ“¥ ì—…ë¡œë“œ ì„œì‹ ë‹¤ìš´ë¡œë“œ',
                    data=buffer,
                    file_name='ê´€ì‹¬ì¢…ëª©_ì„œì‹.xlsx',
                    mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                )

            with col_upload:
                uploaded_file = st.file_uploader(
                    'ê´€ì‹¬ì¢…ëª© ì—‘ì…€ ì—…ë¡œë“œ',
                    type=['xlsx', 'xls'],
                    key='watchlist_upload',
                    help='ì¢…ëª©ì½”ë“œ ì»¬ëŸ¼ì´ í¬í•¨ëœ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”'
                )

            # ì•ˆë‚´ ë¬¸êµ¬ ê°•ì¡°
            st.info('âš ï¸ **ì¢…ëª©ì½”ë“œëŠ” 6ìë¦¬ ìˆ«ì**ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì˜ˆ: 005930, 000660)\n\n'
                   '5930ì²˜ëŸ¼ ì…ë ¥í•´ë„ ìë™ìœ¼ë¡œ 005930ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.')

            # ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬
            if uploaded_file is not None:
                try:
                    watchlist_df = pd.read_excel(uploaded_file)

                    # ì¢…ëª©ì½”ë“œ ì»¬ëŸ¼ ì°¾ê¸°
                    code_col = None
                    for col in watchlist_df.columns:
                        if 'ì¢…ëª©' in col or 'code' in col.lower() or 'ì½”ë“œ' in col:
                            code_col = col
                            break

                    if code_col is None and len(watchlist_df.columns) > 0:
                        code_col = watchlist_df.columns[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©

                    if code_col:
                        # ì¢…ëª©ì½”ë“œ ì¶”ì¶œ ë° 6ìë¦¬ë¡œ ë³€í™˜ (ì•ì— 0 ì±„ìš°ê¸°)
                        codes = watchlist_df[code_col].dropna().astype(str)
                        codes = codes.apply(lambda x: x.split('.')[0])  # ì†Œìˆ˜ì  ì œê±°
                        codes = codes.apply(lambda x: x.zfill(6))  # 6ìë¦¬ë¡œ zero-fill

                        # DB symbol í˜•ì‹ ì²˜ë¦¬ (KRX:000000 ë˜ëŠ” 000000 í˜•ì‹ ëª¨ë‘ ì§€ì›)
                        # symbolì—ì„œ ìˆœìˆ˜ ì½”ë“œë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ
                        db_symbols = search_list_df_original['symbol'].dropna()
                        db_codes_only = db_symbols.apply(lambda x: x.split(':')[-1] if ':' in str(x) else str(x))

                        # ìœ íš¨í•œ ì¢…ëª©ì½”ë“œë§Œ í•„í„°ë§ (DBì— ìˆëŠ” ê²ƒë§Œ)
                        valid_codes = codes[codes.isin(db_codes_only.values)]

                        if not valid_codes.empty:
                            # ì¢…ëª©ì½”ë“œë¡œ ì¢…ëª©ëª… ì°¾ê¸° (DB symbol í˜•ì‹ì— ë§ì¶°ì„œ)
                            matching_mask = db_codes_only.isin(valid_codes.values)
                            watchlist_stocks = search_list_df_original[matching_mask.values]['entity_name'].tolist()

                            st.session_state.selected_stocks = watchlist_stocks

                            st.success(f'âœ… {len(watchlist_stocks)}ê°œ ì¢…ëª©ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.')

                            # ë¡œë“œëœ ì¢…ëª© í‘œì‹œ
                            with st.expander(f'ë¡œë“œëœ ì¢…ëª© ëª©ë¡ ({len(watchlist_stocks)}ê°œ)', expanded=False):
                                loaded_df = search_list_df_original[matching_mask.values][['symbol', 'entity_name', 'mkt']].copy()
                                loaded_df.columns = ['ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'ì‹œì¥']
                                st.dataframe(loaded_df, use_container_width=True, hide_index=True)

                            # ìœ íš¨í•˜ì§€ ì•Šì€ ì½”ë“œ í‘œì‹œ
                            invalid_codes = codes[~codes.isin(db_codes_only.values)]
                            if not invalid_codes.empty:
                                st.warning(f'âš ï¸ {len(invalid_codes)}ê°œ ì¢…ëª©ì½”ë“œê°€ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {", ".join(invalid_codes.head(10).tolist())}')
                        else:
                            st.error('ìœ íš¨í•œ ì¢…ëª©ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ëª©ì½”ë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.')
                    else:
                        st.error('ì¢…ëª©ì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')

                except Exception as e:
                    st.error(f'íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}')

        # ----------------------------------------------------------------------
        # 2. ì´ìŠˆ ì¹´í…Œê³ ë¦¬ í•„í„°
        # ----------------------------------------------------------------------
        col_title2, col_help2 = st.columns([10, 1])
        with col_title2:
            st.subheader('ì´ìŠˆ ì¹´í…Œê³ ë¦¬')
        with col_help2:
            with st.popover('â„¹ï¸'):
                st.markdown('ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ ëœ ê¸°ì‚¬ë¥¼ í•„í„°í•©ë‹ˆë‹¤.')

        col_issue, col_add_keyword = st.columns([1, 1])

        with col_issue:
            selected_issue = st.selectbox(
                'ì¹´í…Œê³ ë¦¬ ì„ íƒ',
                options=list(ISSUE_CATEGORIES.keys()),
                key='filter_issue'
            )

        # ì¹´í…Œê³ ë¦¬ ë³€ê²½ ì‹œ í‚¤ì›Œë“œ ëª©ë¡ ì´ˆê¸°í™”
        if 'last_selected_issue' not in st.session_state or st.session_state.last_selected_issue != selected_issue:
            issue_query = ISSUE_CATEGORIES.get(selected_issue, None)
            if issue_query:
                st.session_state.issue_keywords = issue_query.split(' or ')
                st.session_state.issue_keywords = [kw.strip() for kw in st.session_state.issue_keywords]
            else:
                st.session_state.issue_keywords = []
            st.session_state.last_selected_issue = selected_issue
            # ì¹´í…Œê³ ë¦¬ ë³€ê²½ ì‹œ multiselect ìƒíƒœ ì§ì ‘ ì—…ë°ì´íŠ¸
            st.session_state.issue_keyword_select = st.session_state.issue_keywords.copy()

        # í‚¤ì›Œë“œ ì¶”ê°€ ì½œë°± í•¨ìˆ˜
        def add_keyword_callback():
            keyword = st.session_state.add_issue_keyword
            if keyword:
                if 'issue_keywords' not in st.session_state:
                    st.session_state.issue_keywords = []
                if keyword not in st.session_state.issue_keywords:
                    st.session_state.issue_keywords.append(keyword)
                    # multiselect ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ìƒˆ í‚¤ì›Œë“œ í¬í•¨
                    if 'issue_keyword_select' in st.session_state:
                        current_selected = list(st.session_state.issue_keyword_select)
                        if keyword not in current_selected:
                            current_selected.append(keyword)
                        st.session_state.issue_keyword_select = current_selected
                # ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”
                st.session_state.add_issue_keyword = ''

        with col_add_keyword:
            st.text_input(
                'í‚¤ì›Œë“œ ì¶”ê°€',
                key='add_issue_keyword',
                placeholder='ì˜ˆ: (íš¡ë ¹ and ëŒ€í‘œì´ì‚¬)',
                on_change=add_keyword_callback
            )

        # ì„ íƒëœ í‚¤ì›Œë“œ í‘œì‹œ (ì „ì²´ ì„ íƒ ì‹œ ì œì™¸)
        if selected_issue != 'ì „ì²´' and 'issue_keywords' in st.session_state and st.session_state.issue_keywords:
            selected_keywords = st.multiselect(
                'ì ìš©í•  í‚¤ì›Œë“œ (ìˆ˜ì • ê°€ëŠ¥)',
                options=st.session_state.issue_keywords,
                key='issue_keyword_select'
            )
            st.session_state.selected_issue_keywords = selected_keywords
        else:
            st.session_state.selected_issue_keywords = []

        # ----------------------------------------------------------------------
        # í•„í„° ì ìš© ë²„íŠ¼
        # ----------------------------------------------------------------------
        filter_clicked = st.button('ğŸ” í•„í„° ì ìš©', type='primary', use_container_width=True)

    # ==========================================================================
    # í•„í„° ì‹¤í–‰
    # ==========================================================================
    if filter_clicked:
        # í•„í„° ìœ í˜•ì— ë”°ë¼ ìƒì¥ì‚¬ ëª©ë¡ í•„í„°ë§
        if filter_type == 'ì‹œì¥ë³„ í•„í„°':
            if not selected_market_codes:
                st.error("ì‹œì¥ì„ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                st.stop()
            search_list_df = search_list_df_original[
                search_list_df_original['mkt'].isin(selected_market_codes)
            ]
        elif filter_type == 'ì¢…ëª©ë³„ í•„í„°':
            if 'selected_stocks' not in st.session_state or not st.session_state.selected_stocks:
                st.error("ì¢…ëª©ì„ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                st.stop()
            search_list_df = search_list_df_original[
                search_list_df_original['entity_name'].isin(st.session_state.selected_stocks)
            ]
        else:  # ë‚´ ê´€ì‹¬ì¢…ëª©
            if 'selected_stocks' not in st.session_state or not st.session_state.selected_stocks:
                st.error("ê´€ì‹¬ì¢…ëª© ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                st.stop()
            search_list_df = search_list_df_original[
                search_list_df_original['entity_name'].isin(st.session_state.selected_stocks)
            ]

        if search_list_df.empty:
            st.write("ì„ íƒí•œ ì¡°ê±´ì— ë§ëŠ” ìƒì¥ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
                # ì¡°íšŒìš© Set ìƒì„± (O(1) in ì—°ì‚°ì„ ìœ„í•´)
                symbol_set = set(search_list_df['symbol'].dropna())
                symbol_nice_set = set(search_list_df['symbol_nice'].dropna())
                entity_name_set = set(search_list_df['entity_name'].dropna())
                business_rid_set = set(search_list_df['business_rid'].dropna())
                company_rid_set = set(search_list_df['company_rid'].dropna())

                # ì‹ë³„ì -> ê¸°ì—…ëª… ë§¤í•‘ Dict ìƒì„±
                symbol_to_name = dict(zip(search_list_df['symbol'].dropna(),
                                          search_list_df.loc[search_list_df['symbol'].notna(), 'entity_name']))
                symbol_nice_to_name = dict(zip(search_list_df['symbol_nice'].dropna(),
                                               search_list_df.loc[search_list_df['symbol_nice'].notna(), 'entity_name']))
                business_rid_to_name = dict(zip(search_list_df['business_rid'].dropna(),
                                                search_list_df.loc[search_list_df['business_rid'].notna(), 'entity_name']))
                company_rid_to_name = dict(zip(search_list_df['company_rid'].dropna(),
                                               search_list_df.loc[search_list_df['company_rid'].notna(), 'entity_name']))

                def filter_df(row):
                    """ê° ë¬¸ì„œ í–‰ì—ì„œ KRX ìƒì¥ì‚¬ ì–¸ê¸‰ ì—¬ë¶€ë¥¼ í™•ì¸"""
                    identified_list = []
                    matched = False

                    for col in ['securities', 'entities', 'named_entities']:
                        if col not in row or row[col] is None:
                            continue

                        for entry in row[col]:
                            identified = None

                            if 'symbol' in entry:
                                symbol = entry['symbol']
                                if symbol in symbol_set:
                                    matched = True
                                    identified = symbol_to_name.get(symbol)
                                elif symbol in symbol_nice_set:
                                    matched = True
                                    identified = symbol_nice_to_name.get(symbol)

                            elif 'name' in entry:
                                name = entry['name']
                                if name in entity_name_set:
                                    matched = True
                                    identified = name

                            elif 'business_rid' in entry:
                                brid = entry['business_rid'].replace('-', '')
                                if brid in business_rid_set:
                                    matched = True
                                    identified = business_rid_to_name.get(brid)

                            elif 'company_rid' in entry:
                                crid = entry['company_rid'].replace('-', '')
                                if crid in company_rid_set:
                                    matched = True
                                    identified = company_rid_to_name.get(crid)

                            if identified:
                                identified_list.append(identified)

                    identified_list = list(set(filter(None, identified_list)))
                    return pd.Series([matched, identified_list])

                # ì „ì²´ DataFrameì— í•„í„° í•¨ìˆ˜ ì ìš©
                filtered_df = df.apply(filter_df, axis=1)
                filtered_df.columns = ['matched', 'identified_symbols']

                # ë§¤ì¹­ëœ í–‰ë§Œ ì¶”ì¶œ
                filtered_df2 = df[filtered_df['matched']].copy()
                filtered_df2['identified_symbols'] = filtered_df['identified_symbols']

                # ì´ìŠˆ ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                def check_issue_category(row, issue_query):
                    """ë¬¸ì„œê°€ ì„ íƒëœ ì´ìŠˆ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸"""
                    if issue_query is None:
                        return True

                    text_fields = ['title', 'content', 'description', 'body', 'summary', 'text']
                    text_parts = []
                    for field in text_fields:
                        if field in row and row[field] is not None:
                            text_parts.append(str(row[field]))
                    text = ' '.join(text_parts).lower()

                    conditions = issue_query.split(' or ')
                    for condition in conditions:
                        condition = condition.strip()
                        if condition.startswith('(') and condition.endswith(')'):
                            inner = condition[1:-1]
                            if ' and ' in inner:
                                parts = [p.strip().lower() for p in inner.split(' and ')]
                                if all(p in text for p in parts):
                                    return True
                            else:
                                if inner.lower() in text:
                                    return True
                        else:
                            if condition.lower() in text:
                                return True
                    return False

                # ì´ìŠˆ ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš© (ì‚¬ìš©ìê°€ ì„ íƒí•œ í‚¤ì›Œë“œ ì‚¬ìš©)
                selected_keywords = st.session_state.get('selected_issue_keywords', [])
                if selected_keywords:
                    # ì„ íƒëœ í‚¤ì›Œë“œë¥¼ ì¿¼ë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    issue_query = ' or '.join(selected_keywords)
                    filtered_df2 = filtered_df2[
                        filtered_df2.apply(lambda row: check_issue_category(row, issue_query), axis=1)
                    ]

                # ë§¤ì¹­ëœ ì´ìŠˆ í‚¤ì›Œë“œ ì°¾ê¸°
                def find_matched_issue_keywords(row):
                    """ë¬¸ì„œì—ì„œ ë§¤ì¹­ëœ ì´ìŠˆ í‚¤ì›Œë“œ ì¶”ì¶œ"""
                    matched_kws = []
                    keywords = st.session_state.get('selected_issue_keywords', [])
                    if not keywords:
                        return matched_kws

                    text_fields = ['title', 'content', 'description', 'body', 'summary', 'text']
                    text_parts = []
                    for field in text_fields:
                        if field in row and row[field] is not None:
                            text_parts.append(str(row[field]))
                    text = ' '.join(text_parts).lower()

                    for condition in keywords:
                        condition = condition.strip()
                        if condition.startswith('(') and condition.endswith(')'):
                            inner = condition[1:-1]
                            if ' and ' in inner:
                                parts = [p.strip().lower() for p in inner.split(' and ')]
                                if all(p in text for p in parts):
                                    matched_kws.append(condition)
                            else:
                                if inner.lower() in text:
                                    matched_kws.append(condition)
                        else:
                            if condition.lower() in text:
                                matched_kws.append(condition)
                    return matched_kws

                filtered_df2['matched_keywords'] = filtered_df2.apply(find_matched_issue_keywords, axis=1)

                # ê¸ë¶€ì • ì ìˆ˜ ì¶”ì¶œ (json_normalize í›„ í‰íƒ„í™”ëœ ì»¬ëŸ¼ ì‚¬ìš©)
                # polarity.name, polarity.score ì»¬ëŸ¼ì´ ì´ë¯¸ ì¡´ì¬í•¨
                if 'polarity.name' in filtered_df2.columns:
                    filtered_df2['polarity_name'] = filtered_df2['polarity.name'].fillna('')
                else:
                    filtered_df2['polarity_name'] = ''

                if 'polarity.score' in filtered_df2.columns:
                    filtered_df2['polarity_score'] = filtered_df2['polarity.score'].fillna(0)
                else:
                    filtered_df2['polarity_score'] = 0

                # ê²°ê³¼ë¥¼ session stateì— ì €ì¥ (íŠ¹ì • ì¢…ëª© í•„í„° ì‚¬ìš© ì‹œ ìœ ì§€)
                st.session_state.filtered_df2 = filtered_df2

    # ==========================================================================
    # ê²°ê³¼ í‘œì‹œ (session stateì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œì‹œ)
    # ==========================================================================
    if 'filtered_df2' in st.session_state:
        filtered_df2 = st.session_state.filtered_df2
        result_count = len(filtered_df2)
        if result_count > 0:
            st.success(f"ê²€ìƒ‰ ê²°ê³¼: {len(filtered_df2)}ê±´")

            # --------------------------------------------------------------
            # ì¢…ëª©ë³„ ê¸°ì‚¬ í†µê³„ ì‹œê°í™” (ì›ë³¸ ê²°ê³¼ ê¸°ì¤€)
            # --------------------------------------------------------------
            col_title3, col_help3 = st.columns([10, 1])
            with col_title3:
                st.subheader('ğŸ“Š ì¢…ëª©ë³„ ê¸°ì‚¬ í†µê³„')
            with col_help3:
                with st.popover('â„¹ï¸'):
                    st.markdown('ê¸ë¶€ì •ì ìˆ˜ ë° ì‹ ë¢°ë„ëŠ” DeepSearch ì œê³µ')

            # identified_symbols ë¦¬ìŠ¤íŠ¸ë¥¼ ê°œë³„ í–‰ìœ¼ë¡œ í’€ì–´ì„œ ì§‘ê³„
            stock_counts = {}
            for symbols in filtered_df2['identified_symbols']:
                if symbols:
                    for symbol in symbols:
                        stock_counts[symbol] = stock_counts.get(symbol, 0) + 1

            if stock_counts:
                # ì¢…ëª©ë³„ ê¸ë¶€ì • ì§‘ê³„
                stock_polarity = {}
                for _, row in filtered_df2.iterrows():
                    symbols = row.get('identified_symbols', [])
                    polarity = row.get('polarity_name', '')
                    if symbols:
                        for symbol in symbols:
                            if symbol not in stock_polarity:
                                stock_polarity[symbol] = {'ê¸ì •': 0, 'ì¤‘ë¦½': 0, 'ë¶€ì •': 0, 'ì—†ìŒ': 0}
                            if polarity == 'ê¸ì •':
                                stock_polarity[symbol]['ê¸ì •'] += 1
                            elif polarity == 'ì¤‘ë¦½':
                                stock_polarity[symbol]['ì¤‘ë¦½'] += 1
                            elif polarity == 'ë¶€ì •':
                                stock_polarity[symbol]['ë¶€ì •'] += 1
                            else:
                                stock_polarity[symbol]['ì—†ìŒ'] += 1

                # ì „ì²´ ì¢…ëª© í‘œì‹œ (ê¸°ì‚¬ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ)
                sorted_stocks = sorted(stock_counts.items(), key=lambda x: x[1], reverse=True)
                stock_data = []
                for stock, count in sorted_stocks:
                    pol = stock_polarity.get(stock, {'ê¸ì •': 0, 'ì¤‘ë¦½': 0, 'ë¶€ì •': 0, 'ì—†ìŒ': 0})
                    stock_data.append({
                        'ì¢…ëª©': stock,
                        'ê¸°ì‚¬ìˆ˜': count,
                        'ê¸ì •': pol['ê¸ì •'],
                        'ì¤‘ë¦½': pol['ì¤‘ë¦½'],
                        'ë¶€ì •': pol['ë¶€ì •'],
                        'ì—†ìŒ': pol['ì—†ìŒ']
                    })
                stock_df = pd.DataFrame(stock_data)

                # ì¢…ëª©ë³„ ê¸°ì‚¬ìˆ˜ í…Œì´ë¸”
                st.caption(f'ì¢…ëª©ë³„ ê¸°ì‚¬ìˆ˜ (ì´ {len(stock_df)}ê°œ ì¢…ëª©)')
                # ê¸°ì‚¬ìˆ˜ ìµœëŒ€ê°’ì„ ëª¨ë“  ì»¬ëŸ¼ì˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
                max_count = int(stock_df['ê¸°ì‚¬ìˆ˜'].max()) if not stock_df.empty else 1

                st.dataframe(
                    stock_df,
                    column_config={
                        'ê¸°ì‚¬ìˆ˜': st.column_config.ProgressColumn(
                            'ê¸°ì‚¬ìˆ˜',
                            min_value=0,
                            max_value=max_count,
                            format='%d'
                        ),
                        'ê¸ì •': st.column_config.ProgressColumn(
                            'ê¸ì •',
                            min_value=0,
                            max_value=max_count,
                            format='%d'
                        ),
                        'ì¤‘ë¦½': st.column_config.ProgressColumn(
                            'ì¤‘ë¦½',
                            min_value=0,
                            max_value=max_count,
                            format='%d'
                        ),
                        'ë¶€ì •': st.column_config.ProgressColumn(
                            'ë¶€ì •',
                            min_value=0,
                            max_value=max_count,
                            format='%d'
                        ),
                        'ì—†ìŒ': st.column_config.ProgressColumn(
                            'ì—†ìŒ',
                            min_value=0,
                            max_value=max_count,
                            format='%d'
                        )
                    },
                    use_container_width=True,
                    hide_index=True
                )

            st.divider()

            # --------------------------------------------------------------
            # ê²°ê³¼ í…Œì´ë¸”
            # --------------------------------------------------------------
            st.subheader('ğŸ“° ê²€ìƒ‰ ê²°ê³¼')

            # í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ (ìˆœì„œ: ì›ë¬¸, ì–¸ë¡ ì‚¬, ì„¹ì…˜, ì œëª©, ê´€ë ¨ì¢…ëª©, ë§¤ì¹­í‚¤ì›Œë“œ, ê¸ë¶€ì •, ì‹ ë¢°ë„)
            display_columns = ['content_url', 'publisher', 'section', 'title', 'identified_symbols', 'polarity_name', 'polarity_score']
            if st.session_state.get('selected_issue_keywords', []):
                display_columns.insert(5, 'matched_keywords')

            filtered_df3 = filtered_df2[[col for col in display_columns if col in filtered_df2.columns]]

            # ì»¬ëŸ¼ëª… í•œê¸€í™”
            column_names = {
                'section': 'ì„¹ì…˜',
                'publisher': 'ì–¸ë¡ ì‚¬',
                'title': 'ì œëª©',
                'content': 'ë‚´ìš©',
                'matched_keywords': 'ë§¤ì¹­ í‚¤ì›Œë“œ',
                'polarity_name': 'ê¸ë¶€ì •',
                'polarity_score': 'ì‹ ë¢°ë„',
                'identified_symbols': 'ê´€ë ¨ ì¢…ëª©',
                'content_url': 'ì›ë¬¸'
            }
            filtered_df3 = filtered_df3.rename(columns=column_names)

            # ê¸ë¶€ì • ìƒ‰ìƒ ë§¤í•‘ í•¨ìˆ˜
            def color_polarity(val):
                if val == 'ê¸ì •':
                    return 'background-color: #d4edda; color: #155724'
                elif val == 'ë¶€ì •':
                    return 'background-color: #f8d7da; color: #721c24'
                else:
                    return 'background-color: #fff3cd; color: #856404'

            # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
            st.dataframe(
                filtered_df3.reset_index(drop=True),
                column_config={
                    'ì›ë¬¸': st.column_config.LinkColumn(display_text='ì›ë¬¸'),
                    'ê¸ë¶€ì •': st.column_config.TextColumn(
                        'ê¸ë¶€ì •',
                        help='ê¸ì •/ì¤‘ë¦½/ë¶€ì •'
                    ),
                    'ì‹ ë¢°ë„': st.column_config.ProgressColumn(
                        'ì‹ ë¢°ë„',
                        help='AI ë¶„ì„ ì‹ ë¢°ë„ (0~1)',
                        min_value=0,
                        max_value=1,
                        format='%.2f'
                    )
                },
                use_container_width=True,
                column_order=['ì›ë¬¸', 'ì–¸ë¡ ì‚¬', 'ì„¹ì…˜', 'ì œëª©', 'ê´€ë ¨ ì¢…ëª©', 'ë§¤ì¹­ í‚¤ì›Œë“œ', 'ê¸ë¶€ì •', 'ì‹ ë¢°ë„']
            )

            # --------------------------------------------------------------
            # íŠ¹ì • ì¢…ëª© í•„í„° (ê²€ìƒ‰ ê²°ê³¼ ì•„ë˜ì— ì¶”ê°€ ì„¹ì…˜ìœ¼ë¡œ í‘œì‹œ)
            # --------------------------------------------------------------
            st.divider()
            st.subheader('ğŸ” ì¢…ëª©ë³„ ë‰´ìŠ¤í•„í„° ë° ìµœê·¼ ê³µì‹œ/IR/ì• ë„ë¦¬ìŠ¤íŠ¸ ë³´ê³ ì„œ ë¹„êµí™•ì¸')

            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë°œê²¬ëœ ëª¨ë“  ì¢…ëª© ì¶”ì¶œ
            all_found_stocks = set()
            for symbols in filtered_df2['identified_symbols']:
                if symbols:
                    all_found_stocks.update(symbols)

            all_found_stocks = sorted(list(all_found_stocks))

            if all_found_stocks:
                # ì´ì „ì— ì„ íƒí•œ ì¢…ëª©ì´ í˜„ì¬ ì˜µì…˜ì— ì—†ìœ¼ë©´ ì´ˆê¸°í™”
                if 'result_stock_filter' in st.session_state:
                    current_selection = st.session_state.result_stock_filter
                    if current_selection and current_selection not in all_found_stocks:
                        st.session_state.result_stock_filter = None

                # ë‹¨ì¼ ì„ íƒ ë“œë¡­ë‹¤ìš´ (selectbox)
                stock_options = [''] + all_found_stocks  # ë¹ˆ ì˜µì…˜ ì¶”ê°€
                selected_stock = st.selectbox(
                    'ì¢…ëª© ì„ íƒ',
                    options=stock_options,
                    key='result_stock_filter',
                    format_func=lambda x: 'ì¢…ëª©ì„ ì„ íƒí•˜ì„¸ìš”' if x == '' else x
                )

                # ì„ íƒí•œ ì¢…ëª©ì´ ìˆìœ¼ë©´ ìƒì„¸ ì •ë³´ í‘œì‹œ
                if selected_stock:
                    st.subheader(f'ğŸ“ˆ {selected_stock} ìƒì„¸ ì •ë³´')

                    # ê³µí†µ ë³€ìˆ˜ ì„¤ì •
                    today = datetime.now()
                    one_year_ago = (today - timedelta(days=365)).strftime('%Y-%m-%d')
                    today_str = today.strftime('%Y-%m-%d')

                    # ì¢…ëª© ì‹¬ë³¼ ì¡°íšŒ (ê³µì‹œ/ë³´ê³ ì„œ/ì£¼ê°€ APIìš©)
                    stock_symbol_for_docs = None
                    stock_match_for_docs = search_list_df_original[
                        search_list_df_original['entity_name'] == selected_stock
                    ]
                    if not stock_match_for_docs.empty:
                        stock_symbol_for_docs = stock_match_for_docs.iloc[0]['symbol']

                    # ----------------------------------------------------------
                    # 4ê°œ ì»¬ëŸ¼ ë ˆì´ì•„ì›ƒ: ë‰´ìŠ¤, ê³µì‹œ, IR, ì• ë„ë¦¬ìŠ¤íŠ¸ ë³´ê³ ì„œ
                    # ----------------------------------------------------------
                    col_news, col_disclosure, col_ir, col_research = st.columns(4)

                    # ì»¬ëŸ¼ 1: ë‰´ìŠ¤ ê¸°ì‚¬
                    with col_news:
                        st.markdown('#### ğŸ“° ê´€ë ¨ ë‰´ìŠ¤')
                        st.caption('ìœ„ì˜ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•„í„°í•©ë‹ˆë‹¤')

                        stock_filtered_df = filtered_df2[
                            filtered_df2['identified_symbols'].apply(
                                lambda x: selected_stock in (x or [])
                            )
                        ]

                        if stock_filtered_df.empty:
                            st.info('í•´ë‹¹ ì¢…ëª©ì˜ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.')
                        else:
                            st.caption(f'ì´ {len(stock_filtered_df)}ê±´')
                            news_display = stock_filtered_df[['content_url', 'publisher', 'title', 'polarity_name']].copy()
                            news_display = news_display.rename(columns={
                                'content_url': 'ì›ë¬¸',
                                'publisher': 'ì–¸ë¡ ì‚¬',
                                'title': 'ì œëª©',
                                'polarity_name': 'ê¸ë¶€ì •'
                            })
                            st.dataframe(
                                news_display.reset_index(drop=True),
                                column_config={
                                    'ì›ë¬¸': st.column_config.LinkColumn(display_text='ì›ë¬¸')
                                },
                                hide_index=True,
                                height=400
                            )

                    # ì»¬ëŸ¼ 2: ê³µì‹œ
                    with col_disclosure:
                        st.markdown('#### ğŸ“‹ ìµœê·¼ ê³µì‹œ')
                        st.caption('DeepSearch ì œê³µ, åŸì†ŒìŠ¤: DART')

                        with st.spinner('ê³µì‹œ ì¡°íšŒ ì¤‘...'):
                            if stock_symbol_for_docs:
                                disclosures = get_disclosure_documents(
                                    stock_symbol_for_docs, one_year_ago, today_str, headers, count=50
                                )
                            else:
                                disclosures = []

                        if not disclosures:
                            st.info('ìµœê·¼ 1ë…„ê°„ ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤.')
                        else:
                            st.caption(f'ìµœê·¼ 1ë…„ {len(disclosures)}ê±´')
                            disclosure_data = []
                            for doc in disclosures:
                                disclosure_data.append({
                                    'ì›ë¬¸': doc.get('content_url', '#'),
                                    'ì¼ì': doc.get('created_at', '')[:10] if doc.get('created_at') else '',
                                    'ì œëª©': doc.get('title', 'ì œëª© ì—†ìŒ')
                                })
                            disclosure_df = pd.DataFrame(disclosure_data)
                            st.dataframe(
                                disclosure_df,
                                column_config={
                                    'ì›ë¬¸': st.column_config.LinkColumn(display_text='ì›ë¬¸')
                                },
                                hide_index=True,
                                height=400
                            )

                    # ì»¬ëŸ¼ 3: IR
                    with col_ir:
                        st.markdown('#### ğŸ“¢ ìµœê·¼ IR ìë£Œ')
                        st.caption('DeepSearch ì œê³µ')

                        with st.spinner('IR ì¡°íšŒ ì¤‘...'):
                            if stock_symbol_for_docs:
                                ir_docs = get_ir_documents(
                                    stock_symbol_for_docs, one_year_ago, today_str, headers, count=50
                                )
                            else:
                                ir_docs = []

                        if not ir_docs:
                            st.info('ìµœê·¼ 1ë…„ê°„ ë°°í¬í•œ IRìë£Œê°€ ì—†ìŠµë‹ˆë‹¤. íˆ¬ììì™€ ì†Œí†µì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
                        else:
                            st.caption(f'ìµœê·¼ 1ë…„ {len(ir_docs)}ê±´')
                            ir_data = []
                            for doc in ir_docs:
                                ir_data.append({
                                    'ì›ë¬¸': doc.get('content_url', '#'),
                                    'ì¼ì': doc.get('created_at', '')[:10] if doc.get('created_at') else '',
                                    'ì œëª©': doc.get('title', 'ì œëª© ì—†ìŒ')
                                })
                            ir_df = pd.DataFrame(ir_data)
                            st.dataframe(
                                ir_df,
                                column_config={
                                    'ì›ë¬¸': st.column_config.LinkColumn(display_text='ì›ë¬¸')
                                },
                                hide_index=True,
                                height=400
                            )

                    # ì»¬ëŸ¼ 4: ì• ë„ë¦¬ìŠ¤íŠ¸ ë³´ê³ ì„œ
                    with col_research:
                        st.markdown('#### ğŸ“Š ìµœê·¼ ì• ë„ë¦¬ìŠ¤íŠ¸ ë³´ê³ ì„œ')
                        st.caption('DeepSearch ì œê³µ')

                        with st.spinner('ë³´ê³ ì„œ ì¡°íšŒ ì¤‘...'):
                            if stock_symbol_for_docs:
                                reports = get_analyst_reports(
                                    stock_symbol_for_docs, one_year_ago, today_str, headers, count=50
                                )
                            else:
                                reports = []

                        if not reports:
                            st.info('ìµœê·¼ 1ë…„ê°„ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.')
                        else:
                            st.caption(f'ìµœê·¼ 1ë…„ {len(reports)}ê±´')
                            report_data = []
                            for doc in reports:
                                report_data.append({
                                    'ì›ë¬¸': doc.get('content_url', '#'),
                                    'ì¼ì': doc.get('created_at', '')[:10] if doc.get('created_at') else '',
                                    'ì¦ê¶Œì‚¬': doc.get('publisher', ''),
                                    'ì œëª©': doc.get('title', 'ì œëª© ì—†ìŒ')
                                })
                            report_df = pd.DataFrame(report_data)
                            st.dataframe(
                                report_df,
                                column_config={
                                    'ì›ë¬¸': st.column_config.LinkColumn(display_text='ì›ë¬¸')
                                },
                                hide_index=True,
                                height=400
                            )

                    st.divider()

                    # ----------------------------------------------------------
                    # ì£¼ê°€ ì°¨íŠ¸ ì„¹ì…˜
                    # ----------------------------------------------------------
                    st.markdown('#### ğŸ“ˆ ì£¼ê°€ ì •ë³´')
                    st.caption('DeepSearch ì œê³µ, åŸì†ŒìŠ¤: KOSCOM')

                    # ê¸°ê°„ ì„ íƒ ë¼ë””ì˜¤ ë²„íŠ¼
                    period_options = {
                        '1ê°œì›”': 30,
                        '6ê°œì›”': 180,
                        '52ì£¼': 365
                    }

                    selected_period = st.radio(
                        'ê¸°ê°„ ì„ íƒ',
                        options=list(period_options.keys()),
                        horizontal=True,
                        key='stock_price_period'
                    )

                    # ë‚ ì§œ ê³„ì‚° (todayëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨)
                    date_from_52w = (today - timedelta(days=365)).strftime('%Y-%m-%d')
                    date_to = today.strftime('%Y-%m-%d')

                    # ì¢…ëª©ì´ ë°”ë€Œì—ˆê±°ë‚˜ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ 52ì£¼ ë°ì´í„° ë¡œë“œ
                    if ('loaded_stock' not in st.session_state or
                        st.session_state.loaded_stock != selected_stock or
                        'full_price_df' not in st.session_state):

                        with st.spinner('ì£¼ê°€ ë°ì´í„° ì¡°íšŒ ì¤‘...'):
                            # ì‹¬ë³¼ ì¡°íšŒ (stock_symbol_for_docsëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨)
                            if stock_symbol_for_docs:
                                st.session_state.full_price_df = get_stock_prices(stock_symbol_for_docs, date_from_52w, date_to, headers)
                            else:
                                st.session_state.full_price_df = get_stock_prices(selected_stock, date_from_52w, date_to, headers)
                            st.session_state.loaded_stock = selected_stock

                    # ì„ íƒëœ ê¸°ê°„ì— ë§ê²Œ ë°ì´í„° í•„í„°ë§
                    full_df = st.session_state.full_price_df
                    if not full_df.empty:
                        days_back = period_options[selected_period]
                        cutoff_date = (today - timedelta(days=days_back)).strftime('%Y-%m-%d')
                        if 'date' in full_df.columns:
                            price_df = full_df[full_df['date'].astype(str) >= cutoff_date].copy()
                        else:
                            price_df = full_df.tail(days_back).copy()
                    else:
                        price_df = pd.DataFrame()

                    if not price_df.empty and 'close' in price_df.columns:
                        # ì£¼ê°€ ìš”ì•½ ì •ë³´ (ì°¨íŠ¸ ìœ„ì— í‘œì‹œ)
                        if len(price_df) > 0:
                            latest = price_df.iloc[-1]
                            col_price1, col_price2, col_price3, col_price4 = st.columns(4)
                            with col_price1:
                                st.metric('í˜„ì¬ê°€', f"{int(latest.get('close', 0)):,}ì›")
                            with col_price2:
                                change = latest.get('change', 0)
                                change_rate = latest.get('change_rate', 0)
                                st.metric('ì „ì¼ ëŒ€ë¹„', f"{int(change):,}ì›", f"{change_rate:.2f}%")
                            with col_price3:
                                st.metric('ê³ ê°€', f"{int(latest.get('high', 0)):,}ì›")
                            with col_price4:
                                st.metric('ì €ê°€', f"{int(latest.get('low', 0)):,}ì›")

                        # Plotly ì°¨íŠ¸ ìƒì„±
                        fig = go.Figure()

                        # ìº”ë“¤ìŠ¤í‹± ì°¨íŠ¸
                        # ë‚ ì§œì—ì„œ ì‹œê°„ ë¶€ë¶„ ì œê±° (T00:00:00 ì œê±°)
                        if 'date' in price_df.columns:
                            x_data = price_df['date'].astype(str).str[:10]
                        else:
                            x_data = price_df.index.astype(str).str[:10]

                        # ê¸°ê°„ë³„ ë ˆì´ë¸” í‘œì‹œ ê°„ê²© ì„¤ì •
                        tick_interval = {
                            '1ì£¼ì¼': 1,    # ëª¨ë“  ë ˆì´ë¸” í‘œì‹œ
                            '1ê°œì›”': 5,    # 5ê°œë§ˆë‹¤
                            '6ê°œì›”': 30,   # 30ê°œë§ˆë‹¤
                            '52ì£¼': 60     # 60ê°œë§ˆë‹¤
                        }.get(selected_period, 1)

                        fig.add_trace(go.Candlestick(
                            x=x_data,
                            open=price_df['open'],
                            high=price_df['high'],
                            low=price_df['low'],
                            close=price_df['close'],
                            name='ì£¼ê°€',
                            increasing_line_color='#EF5350',  # ìƒìŠ¹: ë¹¨ê°•
                            decreasing_line_color='#1976D2'   # í•˜ë½: íŒŒë‘
                        ))

                        # ê±°ë˜ëŸ‰ ë°” ì°¨íŠ¸ (ë³´ì¡° ì¶•)
                        if 'volume' in price_df.columns:
                            fig.add_trace(go.Bar(
                                x=x_data,
                                y=price_df['volume'],
                                name='ê±°ë˜ëŸ‰',
                                yaxis='y2',
                                opacity=0.3,
                                marker_color='#aec7e8'
                            ))

                        fig.update_layout(
                            title=f'{selected_stock} ì£¼ê°€ ì°¨íŠ¸ ({selected_period})',
                            xaxis=dict(
                                title='ë‚ ì§œ',
                                type='category',  # ì¹´í…Œê³ ë¦¬ íƒ€ì…: ë°ì´í„° ìˆëŠ” ë‚ ì§œë§Œ í‘œì‹œ
                                dtick=tick_interval,  # ë ˆì´ë¸” í‘œì‹œ ê°„ê²©
                                rangeslider=dict(visible=False)  # ë²”ìœ„ ìŠ¬ë¼ì´ë” ìˆ¨ê¹€
                            ),
                            yaxis=dict(
                                title='ì£¼ê°€ (ì›)',
                                tickformat=','  # ì „ì²´ ìˆ«ì í‘œì‹œ (k ì•½ì–´ ì‚¬ìš© ì•ˆí•¨)
                            ),
                            yaxis2=dict(
                                title='ê±°ë˜ëŸ‰',
                                overlaying='y',
                                side='right',
                                showgrid=False,
                                tickformat='.2s'  # M ë‹¨ìœ„ ì‚¬ìš©
                            ),
                            hovermode='x unified',
                            height=400,
                            legend=dict(
                                orientation='h',
                                yanchor='bottom',
                                y=1.02,
                                xanchor='right',
                                x=1
                            )
                        )

                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info('ì£¼ê°€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')

            else:
                st.info("ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ì¢…ëª©ì´ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            st.warning("í•„í„° ì¡°ê±´ì— ë§ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

else:
    st.write("ë¨¼ì € 'ê²€ìƒ‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")


# ==============================================================================
# ì‹¤í–‰ ë°©ë²• (ì°¸ê³ ìš© ì£¼ì„)
# ==============================================================================
# [ë¡œì»¬ ì‹¤í–‰]
# 1. ê°€ìƒí™˜ê²½ í™œì„±í™”:
#    python -m venv venv
#    venv\Scripts\activate  # Windows
#    source venv/bin/activate  # Mac/Linux
#
# 2. ì˜ì¡´ì„± ì„¤ì¹˜:
#    pip install -r requirements.txt
#
# 3. í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼ ìƒì„±):
#    API_KEY=your_deepsearch_api_key
#    DB_HOST=db.xxxxx.supabase.co
#    DB_PORT=5432
#    DB_NAME=postgres
#    DB_USER=your_db_user
#    DB_PASSWORD=your_db_password
#
# 4. ì‹¤í–‰:
#    streamlit run deepsearch_query.py
#
# [Streamlit Cloud ë°°í¬]
# 1. GitHub ì €ì¥ì†Œì— ì½”ë“œ í‘¸ì‹œ
# 2. Streamlit Cloudì—ì„œ ì•± ìƒì„±
# 3. Secrets ì„¤ì • (secrets.toml í˜•ì‹):
#    [general]
#    api_key = "your_api_key"
#    db_user = "your_db_user"
#    db_password = "your_db_password"
#    db_host = "db.xxxxx.supabase.co"
#    db_port = "5432"
#    db_name = "postgres"
#
#    [crud]
#    db_user = "your_crud_user"
#    db_password = "your_crud_password"
#    db_host = "db.xxxxx.supabase.co"
#    db_port = "5432"
